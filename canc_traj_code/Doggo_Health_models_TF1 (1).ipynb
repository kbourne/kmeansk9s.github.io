{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.local/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
      "  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.9.0 imblearn-0.0\n",
      "Requirement already satisfied: pandas==1.1.5 in ./.local/lib/python3.7/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.5) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.5) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./.local/lib/python3.7/site-packages (from pandas==1.1.5) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.16.0)\n",
      "Requirement already satisfied: tensorflow==1.13.1 in ./.local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./.local/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.19.5)\n",
      "Requirement already satisfied: astor>=0.6.0 in ./.local/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in ./.local/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.37.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./.local/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in ./.local/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.13.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.46.3)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.7)\n",
      "Requirement already satisfied: mock>=2.0.0 in ./.local/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.8.0)\n",
      "Requirement already satisfied: numpy==1.19.5 in ./.local/lib/python3.7/site-packages (1.19.5)\n",
      "Requirement already satisfied: hmmlearn in ./.local/lib/python3.7/site-packages (0.2.7)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/conda/lib/python3.7/site-packages (from hmmlearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.7/site-packages (from hmmlearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.10 in ./.local/lib/python3.7/site-packages (from hmmlearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# need to set some versions\n",
    "import sys\n",
    "!pip install imblearn\n",
    "\n",
    "# For Google Colab:\n",
    "#Mounting drive to access datafile\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# for Google Workbook:\n",
    "!{sys.executable} -m pip install --user pandas==1.1.5\n",
    "!{sys.executable} -m pip install --user tensorflow==1.13.1 # 1.15.0 # paper said us 1.10.0, but it doesn't exist???\n",
    "!{sys.executable} -m pip install --user numpy==1.19.5  # this one breaks on the tensor to numpy issue: 1.16.0 # tested with 1.16.0 # paper said us 1.15.3, but too many dependency conflicts\n",
    "!{sys.executable} -m pip install --user hmmlearn\n",
    "\n",
    "# for other environments:\n",
    "# !{sys.executable} -m pip install pandas==1.1.5\n",
    "# !{sys.executable} -m pip install tensorflow==1.13.1 # 1.15.0 # paper said us 1.10.0, but it doesn't exist???\n",
    "# !{sys.executable} -m pip install numpy==1.19.5  # this one breaks on the tensor to numpy issue: 1.16.0 # tested with 1.16.0 # paper said us 1.15.3, but too many dependency conflicts\n",
    "# !{sys.executable} -m pip install hmmlearn\n",
    "\n",
    "# may have to restart kernel after running these to get access to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing/installing modules\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import itertools\n",
    "import functools\n",
    "import inspect\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import PhasedLSTMCell, MultiRNNCell, BasicRNNCell\n",
    "from tensorflow.python.ops import rnn_cell, rnn\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op, dtypes, ops, tensor_shape, tensor_util   \n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import * \n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training import checkpointable\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal\n",
    "from hmmlearn import hmm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version - should be 3.6 or could have issues\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull up data for EDA\n",
    "heal_df = pd.read_csv(\"CSV_DATA/DAP_2020_HLES_health_condition_v1.1.csv\")\n",
    "ownr_df = pd.read_csv(\"CSV_DATA/DAP_2020_HLES_dog_owner_v1.1.csv\")\n",
    "cncr_df = pd.read_csv(\"CSV_DATA/DAP_2020_HLES_cancer_condition_v1.1.csv\")\n",
    "\n",
    "# if in Google Colab:\n",
    "# heal_df = pd.read_csv(\"/content/drive/Shared drives/CAPSTONE/CSV_DATA/DAP_2020_HLES_health_condition_v1.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINOR EDA\n",
    "print(len(heal_df))\n",
    "print(list(heal_df.columns))\n",
    "heal_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to know how many records per dog\n",
    "# going to handle each entry as a separate visit for now, \n",
    "# but it looks like it is an entry per disease, so should massage later\n",
    "# When come back to this: assume that if same month and same year, \n",
    "# but different diseases, it was during the same visit\n",
    "heal_df.groupby('dog_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4\n",
    "# reduce to just dogs with 4+ health records\n",
    "# then look at the most common conditions & most common surgical/hospitalization scenarios\n",
    "sm_hl_df = heal_df[['dog_id', 'hs_condition_type','hs_required_surgery_or_hospitalization']]\n",
    "# sm_hl_df.groupby('hs_condition_type').count()\n",
    "# reduce to only dogs with seq_lenth+ of records\n",
    "segmin_df = sm_hl_df[sm_hl_df.groupby('dog_id')['dog_id'].transform('size') > seq_length]\n",
    "print(len(segmin_df))\n",
    "# most common conditions\n",
    "segmin_df.groupby('hs_condition_type').count()\n",
    "# most common are (in order):\n",
    "# 4, 3, 18, 16, 11, 1, etc.\n",
    "# skin - 4 - 4990\n",
    "# mouth/dental/oral - 3 - 4837\n",
    "# trauma - 18 - 3231\n",
    "# infection/parasites - 16 - 3197\n",
    "# bone/orthoepedic - 11 - 2837\n",
    "# eye - 1 - 2170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "kicked = heal_df.loc[(heal_df['hs_condition']==1811),['dog_id', 'hs_condition_type','hs_required_surgery_or_hospitalization']]\n",
    "kicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083de0f-5579-4537-900c-56dadc60fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin = heal_df.loc[(heal_df['hs_condition_type']==4),['dog_id', 'hs_condition','hs_required_surgery_or_hospitalization']]\n",
    "skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b8f8d-3c97-41d2-9803-3a77d84d4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin.groupby('hs_condition').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just looking at skin issues - want it broken down by hospitalized, surgery, both, none\n",
    "skin1_df = heal_df[['dog_id', 'hs_condition_type','hs_required_surgery_or_hospitalization']]\n",
    "# sm_hl_df.groupby('hs_condition_type').count()\n",
    "# reduce to only dogs with seq_lenth+ of records\n",
    "# segmin_df = skin1_df[skin1_df.groupby('dog_id')['dog_id'].transform('size') > seq_length]\n",
    "print(len(skin1_df))\n",
    "skin2_df = skin1_df.loc[(skin1_df['hs_condition_type']==3),['dog_id', 'hs_condition_type','hs_required_surgery_or_hospitalization']]\n",
    "# most common conditions\n",
    "skin2_df.groupby('hs_required_surgery_or_hospitalization').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common surgical/hospitalization scenarios\n",
    "segmin_df.groupby('hs_required_surgery_or_hospitalization').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ca8ce-1495-4062-a45b-b57da866be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cncr_df))\n",
    "print(list(cncr_df.columns))\n",
    "cncr_df # .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab4739-d820-4c4f-8e0b-a7d9af88bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ownr_df))\n",
    "print(list(ownr_df.columns))\n",
    "ownr_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-spectrum",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Time-series Generative Adversarial Networks - MODEL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TimeGANs key functionality\n",
    "## Min Max Normalizer\n",
    "\n",
    "def MinMaxScaler2(dataX):\n",
    "    \n",
    "    min_val = np.min(np.min(dataX, axis = 0), axis = 0)\n",
    "    dataX = dataX - min_val\n",
    "    \n",
    "    max_val = np.max(np.max(dataX, axis = 0), axis = 0)\n",
    "    dataX = dataX / (max_val + 1e-7)\n",
    "    \n",
    "    return dataX, min_val, max_val\n",
    "\n",
    "## Start TimeGAN function (Input: Original data, Output: Synthetic Data)\n",
    "\n",
    "def timegan (dataX, parameters):\n",
    "  \n",
    "    # Initialization on the Graph\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    # Basic Parameters\n",
    "    No = len(dataX)\n",
    "    data_dim = len(dataX[0][0,:])\n",
    "    \n",
    "    # Maximum seq length and each seq length\n",
    "    dataT = list()\n",
    "    Max_Seq_Len = 0\n",
    "    for i in range(No):\n",
    "        Max_Seq_Len = max(Max_Seq_Len, len(dataX[i][:,0]))\n",
    "        dataT.append(len(dataX[i][:,0]))\n",
    "        \n",
    "    # Normalization\n",
    "    if ((np.max(dataX) > 1) | (np.min(dataX) < 0)):\n",
    "        dataX, min_val, max_val = MinMaxScaler2(dataX)\n",
    "        Normalization_Flag = 1\n",
    "    else:\n",
    "        Normalization_Flag = 0\n",
    "     \n",
    "    # Network Parameters\n",
    "    hidden_dim   = parameters['hidden_dim'] \n",
    "    num_layers   = parameters['num_layers']\n",
    "    iterations   = parameters['iterations']\n",
    "    batch_size   = parameters['batch_size']\n",
    "    module_name  = parameters['module_name']    # 'lstm' or 'lstmLN'\n",
    "    z_dim        = parameters['z_dim']\n",
    "    gamma        = 1\n",
    "    \n",
    "    ## input place holders\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    X = tf.compat.v1.placeholder(tf.float32, [None, Max_Seq_Len, data_dim], name = \"myinput_x\")\n",
    "    Z = tf.compat.v1.placeholder(tf.float32, [None, Max_Seq_Len, z_dim], name = \"myinput_z\")\n",
    "    T = tf.compat.v1.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
    "    \n",
    "    ## Basic RNN Cell\n",
    "          \n",
    "    def rnn_cell(module_name):\n",
    "      # GRU\n",
    "        if (module_name == 'gru'):\n",
    "            rnn_cell = tf.compat.v1.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "      # LSTM\n",
    "        elif (module_name == 'lstm'):\n",
    "            rnn_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "      # LSTM Layer Normalization\n",
    "        elif (module_name == 'lstmLN'):\n",
    "            rnn_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "        return rnn_cell\n",
    "      \n",
    "        \n",
    "    ## build a RNN embedding network      \n",
    "    \n",
    "    def embedder (X, T):      \n",
    "      \n",
    "        with tf.compat.v1.variable_scope(\"embedder\", reuse = tf.compat.v1.AUTO_REUSE):\n",
    "            \n",
    "            e_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name) for _ in range(num_layers)])\n",
    "                \n",
    "            e_outputs, e_last_states = tf.compat.v1.nn.dynamic_rnn(e_cell, X, dtype=tf.float32, sequence_length = T)\n",
    "            \n",
    "            H = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "        return H\n",
    "      \n",
    "    ##### Recovery\n",
    "    \n",
    "    def recovery (H, T):      \n",
    "      \n",
    "        with tf.compat.v1.variable_scope(\"recovery\", reuse = tf.compat.v1.AUTO_REUSE):       \n",
    "              \n",
    "            r_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name) for _ in range(num_layers)])\n",
    "                \n",
    "            r_outputs, r_last_states = tf.compat.v1.nn.dynamic_rnn(r_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "            \n",
    "            X_tilde = tf.contrib.layers.fully_connected(r_outputs, data_dim, activation_fn=tf.nn.sigmoid) \n",
    "\n",
    "        return X_tilde\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## build a RNN generator network\n",
    "    \n",
    "    def generator (Z, T):      \n",
    "      \n",
    "        with tf.compat.v1.variable_scope(\"generator\", reuse = tf.compat.v1.AUTO_REUSE):\n",
    "            \n",
    "            e_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name) for _ in range(num_layers)])\n",
    "                \n",
    "            e_outputs, e_last_states = tf.compat.v1.nn.dynamic_rnn(e_cell, Z, dtype=tf.float32, sequence_length = T)\n",
    "            \n",
    "            E = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
    "\n",
    "        return E\n",
    "      \n",
    "    def supervisor (H, T):      \n",
    "      \n",
    "        with tf.compat.v1.variable_scope(\"supervisor\", reuse = tf.compat.v1.AUTO_REUSE):\n",
    "            \n",
    "            e_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name) for _ in range(num_layers-1)])\n",
    "                \n",
    "            e_outputs, e_last_states = tf.compat.v1.nn.dynamic_rnn(e_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "            \n",
    "            S = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
    "\n",
    "        return S\n",
    "      \n",
    "      \n",
    "      \n",
    "    ## builde a RNN discriminator network \n",
    "    \n",
    "    def discriminator (H, T):\n",
    "      \n",
    "        with tf.compat.v1.variable_scope(\"discriminator\", reuse = tf.compat.v1.AUTO_REUSE):\n",
    "            \n",
    "            d_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name) for _ in range(num_layers)])\n",
    "                \n",
    "            d_outputs, d_last_states = tf.compat.v1.nn.dynamic_rnn(d_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "            \n",
    "            Y_hat = tf.contrib.layers.fully_connected(d_outputs, 1, activation_fn=None) \n",
    "    \n",
    "        return Y_hat   \n",
    "    \n",
    "    \n",
    "    ## Random vector generation\n",
    "    def random_generator (batch_size, z_dim, T_mb, Max_Seq_Len):\n",
    "      \n",
    "        Z_mb = list()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            Temp = np.zeros([Max_Seq_Len, z_dim])\n",
    "            \n",
    "            Temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n",
    "        \n",
    "            Temp[:T_mb[i],:] = Temp_Z\n",
    "            \n",
    "            Z_mb.append(Temp_Z)\n",
    "      \n",
    "        return Z_mb\n",
    "    \n",
    "    ## Functions\n",
    "    \n",
    "    # Embedder Networks\n",
    "    H = embedder(X, T)\n",
    "    X_tilde = recovery(H, T)\n",
    "    \n",
    "    # Generator\n",
    "    E_hat = generator(Z, T)\n",
    "    H_hat = supervisor(E_hat, T)\n",
    "    H_hat_supervise = supervisor(H, T)\n",
    "    \n",
    "    # Synthetic data\n",
    "    X_hat = recovery(H_hat, T)\n",
    "    \n",
    "    # Discriminator\n",
    "    Y_fake = discriminator(H_hat, T)\n",
    "    Y_real = discriminator(H, T)     \n",
    "    Y_fake_e = discriminator(E_hat, T)\n",
    "    \n",
    "    # Variables        \n",
    "    e_vars = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('embedder')]\n",
    "    r_vars = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('recovery')]\n",
    "    g_vars = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('generator')]\n",
    "    s_vars = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('supervisor')]\n",
    "    d_vars = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('discriminator')]\n",
    "    \n",
    "    # Loss for the discriminator\n",
    "    D_loss_real = tf.compat.v1.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)\n",
    "    D_loss_fake = tf.compat.v1.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)\n",
    "    D_loss_fake_e = tf.compat.v1.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)\n",
    "    D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "            \n",
    "    # Loss for the generator\n",
    "    # 1. Adversarial loss\n",
    "    G_loss_U = tf.compat.v1.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake), Y_fake)\n",
    "    G_loss_U_e = tf.compat.v1.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e), Y_fake_e)\n",
    "    \n",
    "    # 2. Supervised loss\n",
    "    G_loss_S = tf.compat.v1.losses.mean_squared_error(H[:,1:,:], H_hat_supervise[:,1:,:])\n",
    "    \n",
    "    # 3. Two Momments\n",
    "    G_loss_V1 = tf.reduce_mean(input_tensor=np.abs(tf.sqrt(tf.nn.moments(x=X_hat,axes=[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(x=X,axes=[0])[1] + 1e-6)))\n",
    "    G_loss_V2 = tf.reduce_mean(input_tensor=np.abs((tf.nn.moments(x=X_hat,axes=[0])[0]) - (tf.nn.moments(x=X,axes=[0])[0])))\n",
    "\n",
    "    G_loss_V = G_loss_V1 + G_loss_V2\n",
    "    \n",
    "    # Summation\n",
    "    G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V \n",
    "            \n",
    "    # Loss for the embedder network\n",
    "    E_loss_T0 = tf.compat.v1.losses.mean_squared_error(X, X_tilde)\n",
    "    E_loss0 = 10*tf.sqrt(E_loss_T0)\n",
    "    E_loss = E_loss0  + 0.1*G_loss_S\n",
    "    \n",
    "    # optimizer\n",
    "    E0_solver = tf.compat.v1.train.AdamOptimizer().minimize(E_loss0, var_list = e_vars + r_vars)\n",
    "    E_solver = tf.compat.v1.train.AdamOptimizer().minimize(E_loss, var_list = e_vars + r_vars)\n",
    "    D_solver = tf.compat.v1.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
    "    G_solver = tf.compat.v1.train.AdamOptimizer().minimize(G_loss, var_list = g_vars + s_vars)      \n",
    "    GS_solver = tf.compat.v1.train.AdamOptimizer().minimize(G_loss_S, var_list = g_vars + s_vars)   \n",
    "        \n",
    "    ## Sessions    \n",
    "    \n",
    "    sess = tf.compat.v1.Session()\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    ## Embedding Learning\n",
    "    \n",
    "    print('Start Embedding Network Training')\n",
    "    \n",
    "    for itt in range(iterations):\n",
    "        \n",
    "        # Batch setting\n",
    "        idx = np.random.permutation(No)\n",
    "        train_idx = idx[:batch_size]     \n",
    "            \n",
    "        X_mb = list(dataX[i] for i in train_idx)\n",
    "        T_mb = list(dataT[i] for i in train_idx)\n",
    "            \n",
    "        # Train embedder        \n",
    "        _, step_e_loss = sess.run([E0_solver, E_loss_T0], feed_dict={X: X_mb, T: T_mb})\n",
    "        \n",
    "        if itt % 1000 == 0:\n",
    "            print('step: '+ str(itt) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss),4)) )        \n",
    "            \n",
    "    print('Finish Embedding Network Training')\n",
    "    \n",
    "    ## Training Supervised Loss First\n",
    "    \n",
    "    print('Start Training with Supervised Loss Only')\n",
    "    \n",
    "    for itt in range(iterations):\n",
    "        \n",
    "        # Batch setting\n",
    "        idx = np.random.permutation(No)\n",
    "        train_idx = idx[:batch_size]     \n",
    "            \n",
    "        X_mb = list(dataX[i] for i in train_idx)\n",
    "        T_mb = list(dataT[i] for i in train_idx)        \n",
    "        \n",
    "        Z_mb = random_generator(batch_size, z_dim, T_mb, Max_Seq_Len)\n",
    "        \n",
    "        # Train generator       \n",
    "        _, step_g_loss_s = sess.run([GS_solver, G_loss_S], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})\n",
    "                           \n",
    "        if itt % 1000 == 0:\n",
    "            print('step: '+ str(itt) + ', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s),4)) )\n",
    "                \n",
    "    print('Finish Training with Supervised Loss Only')\n",
    "    \n",
    "    ## Joint Training\n",
    "    \n",
    "    print('Start Joint Training')\n",
    "    \n",
    "    # Training step\n",
    "    for itt in range(iterations):\n",
    "      \n",
    "        # Generator Training\n",
    "        for kk in range(2):\n",
    "          \n",
    "            # Batch setting\n",
    "            idx = np.random.permutation(No)\n",
    "            train_idx = idx[:batch_size]     \n",
    "            \n",
    "            X_mb = list(dataX[i] for i in train_idx)\n",
    "            T_mb = list(dataT[i] for i in train_idx)\n",
    "            \n",
    "            # Random vector generation\n",
    "            Z_mb = random_generator(batch_size, z_dim, T_mb, Max_Seq_Len)\n",
    "              \n",
    "            # Train generator\n",
    "            _, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run([G_solver, G_loss_U, G_loss_S, G_loss_V], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})\n",
    "            \n",
    "            # Train embedder        \n",
    "            _, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})   \n",
    "           \n",
    "        ## Discriminator Training\n",
    "        \n",
    "        # Batch setting\n",
    "        idx = np.random.permutation(No)\n",
    "        train_idx = idx[:batch_size]     \n",
    "        \n",
    "        X_mb = list(dataX[i] for i in train_idx)\n",
    "        T_mb = list(dataT[i] for i in train_idx)\n",
    "        \n",
    "        # Random vector generation\n",
    "        Z_mb = random_generator(batch_size, z_dim, T_mb, Max_Seq_Len)\n",
    "            \n",
    "        \n",
    "        check_d_loss = sess.run(D_loss, feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
    "        \n",
    "        # Train discriminator\n",
    "        \n",
    "        if (check_d_loss > 0.15):        \n",
    "            _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
    "        \n",
    "        ## Checkpoints\n",
    "        if itt % 1000 == 0:\n",
    "            print('step: '+ str(itt) + \n",
    "                  ', d_loss: ' + str(np.round(step_d_loss,4)) + \n",
    "                  ', g_loss_u: ' + str(np.round(step_g_loss_u,4)) + \n",
    "                  ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s),4)) + \n",
    "                  ', g_loss_v: ' + str(np.round(step_g_loss_v,4)) + \n",
    "                  ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0),4))  )\n",
    "   \n",
    "    \n",
    "    print('Finish Joint Training')\n",
    "    \n",
    "    ## Final Outputs\n",
    "    \n",
    "    Z_mb = random_generator(No, z_dim, dataT, Max_Seq_Len)\n",
    "    \n",
    "    X_hat_curr = sess.run(X_hat, feed_dict={Z: Z_mb, X: dataX, T: dataT})    \n",
    "    \n",
    "    ## List of the final outputs\n",
    "    \n",
    "    dataX_hat = list()\n",
    "    \n",
    "    for i in range(No):\n",
    "        Temp = X_hat_curr[i,:dataT[i],:]\n",
    "        dataX_hat.append(Temp)\n",
    "        \n",
    "    # Renormalization\n",
    "    if (Normalization_Flag == 1):\n",
    "        dataX_hat = dataX_hat * max_val\n",
    "        dataX_hat = dataX_hat + min_val\n",
    "    \n",
    "    return dataX_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-hoc RNN Classifier \n",
    "\n",
    "def discriminative_score_metrics (dataX, dataX_hat):\n",
    "  \n",
    "    # Initialization on the Graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Basic Parameters\n",
    "    No = len(dataX)\n",
    "    data_dim = len(dataX[0][0,:])\n",
    "    \n",
    "    # Compute Maximum seq length and each seq length\n",
    "    dataT = list()\n",
    "    Max_Seq_Len = 0\n",
    "    for i in range(No):\n",
    "        Max_Seq_Len = max(Max_Seq_Len, len(dataX[i][:,0]))\n",
    "        dataT.append(len(dataX[i][:,0]))\n",
    "     \n",
    "    # Network Parameters\n",
    "    hidden_dim = max(int(data_dim/2),1)\n",
    "    iterations = 2000\n",
    "    batch_size = 128\n",
    "    \n",
    "    ## input place holders\n",
    "    # Features\n",
    "    X = tf.placeholder(tf.float32, [None, Max_Seq_Len, data_dim], name = \"myinput_x\")\n",
    "    X_hat = tf.placeholder(tf.float32, [None, Max_Seq_Len, data_dim], name = \"myinput_x_hat\")\n",
    "    \n",
    "    # Times\n",
    "    T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
    "    T_hat = tf.placeholder(tf.int32, [None], name = \"myinput_t_hat\")\n",
    "    \n",
    "    ## builde a RNN classification network \n",
    "    \n",
    "    def discriminator (X, T):\n",
    "      \n",
    "        with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE) as vs:\n",
    "            \n",
    "            d_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'cd_cell')\n",
    "                    \n",
    "            d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, X, dtype=tf.float32, sequence_length = T)\n",
    "                \n",
    "            # Logits\n",
    "            Y_hat = tf.contrib.layers.fully_connected(d_last_states, 1, activation_fn=None) \n",
    "            \n",
    "            # Sigmoid output\n",
    "            Y_hat_Final = tf.nn.sigmoid(Y_hat)\n",
    "            \n",
    "            # Variables\n",
    "            d_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
    "    \n",
    "        return Y_hat, Y_hat_Final, d_vars\n",
    "    \n",
    "    ## Train / Test Division\n",
    "    def train_test_divide (dataX, dataX_hat, dataT):\n",
    "      \n",
    "        # Divide train/test index\n",
    "        No = len(dataX)\n",
    "        idx = np.random.permutation(No)\n",
    "        train_idx = idx[:int(No*0.8)]\n",
    "        test_idx = idx[int(No*0.8):]\n",
    "        \n",
    "        # Train and Test X\n",
    "        trainX = [dataX[i] for i in train_idx]\n",
    "        trainX_hat = [dataX_hat[i] for i in train_idx]\n",
    "        \n",
    "        testX = [dataX[i] for i in test_idx]\n",
    "        testX_hat = [dataX_hat[i] for i in test_idx]\n",
    "        \n",
    "        # Train and Test T\n",
    "        trainT = [dataT[i] for i in train_idx]\n",
    "        testT = [dataT[i] for i in test_idx]\n",
    "      \n",
    "        return trainX, trainX_hat, testX, testX_hat, trainT, testT\n",
    "    \n",
    "    ## Functions\n",
    "    # Variables\n",
    "    Y_real, Y_pred_real, d_vars = discriminator(X, T)\n",
    "    Y_fake, Y_pred_fake, _ = discriminator(X_hat, T_hat)\n",
    "        \n",
    "    # Loss for the discriminator\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_real, labels = tf.ones_like(Y_real)))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_fake, labels = tf.zeros_like(Y_fake)))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    \n",
    "    # optimizer\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
    "        \n",
    "    ## Sessions    \n",
    "\n",
    "    # Start session and initialize\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train / Test Division\n",
    "    trainX, trainX_hat, testX, testX_hat, trainT, testT = train_test_divide (dataX, dataX_hat, dataT)\n",
    "    \n",
    "    # Training step\n",
    "    for itt in range(iterations):\n",
    "          \n",
    "        # Batch setting\n",
    "        idx = np.random.permutation(len(trainX))\n",
    "        train_idx = idx[:batch_size]     \n",
    "            \n",
    "        X_mb = list(trainX[i] for i in train_idx)\n",
    "        T_mb = list(trainT[i] for i in train_idx)\n",
    "        \n",
    "        # Batch setting\n",
    "        idx = np.random.permutation(len(trainX_hat))\n",
    "        train_idx = idx[:batch_size]     \n",
    "            \n",
    "        X_hat_mb = list(trainX_hat[i] for i in train_idx)\n",
    "        T_hat_mb = list(trainT[i] for i in train_idx)\n",
    "          \n",
    "        # Train discriminator\n",
    "        _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, X_hat: X_hat_mb, T_hat: T_hat_mb})            \n",
    "        \n",
    "        ## Checkpoints\n",
    "#        if itt % 500 == 0:\n",
    "#            print(\"[step: {}] loss - d loss: {}\".format(itt, np.round(step_d_loss,4)))\n",
    "    \n",
    "    ## Final Outputs (ontTesting set)\n",
    "    \n",
    "    Y_pred_real_curr, Y_pred_fake_curr = sess.run([Y_pred_real, Y_pred_fake], feed_dict={X: testX, T: testT, X_hat: testX_hat, T_hat: testT})\n",
    "    \n",
    "    Y_pred_final = np.squeeze(np.concatenate((Y_pred_real_curr, Y_pred_fake_curr), axis = 0))\n",
    "    Y_label_final = np.concatenate((np.ones([len(Y_pred_real_curr),]), np.zeros([len(Y_pred_real_curr),])), axis = 0)\n",
    "    \n",
    "    ## Accuracy\n",
    "    Acc = accuracy_score(Y_label_final, Y_pred_final>0.5)\n",
    "    \n",
    "    Disc_Score = np.abs(0.5-Acc)\n",
    "    \n",
    "    return Disc_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18288c-4a40-4cd6-9168-b17a1cd74b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TimeGANs Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA Analysis\n",
    "    \n",
    "def PCA_Analysis (dataX, dataX_hat):\n",
    "\n",
    "    # Analysis Data Size\n",
    "    Sample_No = 1000\n",
    "    \n",
    "    # Data Preprocessing\n",
    "    for i in range(Sample_No):\n",
    "        if (i == 0):\n",
    "            arrayX = np.reshape(np.mean(np.asarray(dataX[0]),1), [1,len(dataX[0][:,0])])\n",
    "            arrayX_hat = np.reshape(np.mean(np.asarray(dataX_hat[0]),1), [1,len(dataX[0][:,0])])\n",
    "        else:\n",
    "            arrayX = np.concatenate((arrayX, np.reshape(np.mean(np.asarray(dataX[i]),1), [1,len(dataX[0][:,0])])))\n",
    "            arrayX_hat = np.concatenate((arrayX_hat, np.reshape(np.mean(np.asarray(dataX_hat[i]),1), [1,len(dataX[0][:,0])])))\n",
    "    \n",
    "    # Parameters        \n",
    "    No = len(arrayX[:,0])\n",
    "    colors = [\"red\" for i in range(No)] +  [\"blue\" for i in range(No)]    \n",
    "    \n",
    "    # PCA Analysis\n",
    "    pca = PCA(n_components = 2)\n",
    "    pca.fit(arrayX)\n",
    "    pca_results = pca.transform(arrayX)\n",
    "    pca_hat_results = pca.transform(arrayX_hat)\n",
    "        \n",
    "    # Plotting\n",
    "    f, ax = plt.subplots(1)\n",
    "    \n",
    "    plt.scatter(pca_results[:,0], pca_results[:,1], c = colors[:No], alpha = 0.2, label = \"Original\")\n",
    "    plt.scatter(pca_hat_results[:,0], pca_hat_results[:,1], c = colors[No:], alpha = 0.2, label = \"Synthetic\")\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.title('PCA plot')\n",
    "    plt.xlabel('x-pca')\n",
    "    plt.ylabel('y_pca')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "## TSNE Analysis\n",
    "    \n",
    "def tSNE_Analysis (dataX, dataX_hat):\n",
    "  \n",
    "    # Analysis Data Size\n",
    "    Sample_No = 1000\n",
    "  \n",
    "    # Preprocess\n",
    "    for i in range(Sample_No):\n",
    "        if (i == 0):\n",
    "            arrayX = np.reshape(np.mean(np.asarray(dataX[0]),1), [1,len(dataX[0][:,0])])\n",
    "            arrayX_hat = np.reshape(np.mean(np.asarray(dataX_hat[0]),1), [1,len(dataX[0][:,0])])\n",
    "        else:\n",
    "            arrayX = np.concatenate((arrayX, np.reshape(np.mean(np.asarray(dataX[i]),1), [1,len(dataX[0][:,0])])))\n",
    "            arrayX_hat = np.concatenate((arrayX_hat, np.reshape(np.mean(np.asarray(dataX_hat[i]),1), [1,len(dataX[0][:,0])])))\n",
    "     \n",
    "    # Do t-SNE Analysis together       \n",
    "    final_arrayX = np.concatenate((arrayX, arrayX_hat), axis = 0)\n",
    "    \n",
    "    # Parameters\n",
    "    No = len(arrayX[:,0])\n",
    "    colors = [\"red\" for i in range(No)] +  [\"blue\" for i in range(No)]    \n",
    "    \n",
    "    # TSNE anlaysis\n",
    "    tsne = TSNE(n_components = 2, verbose = 1, perplexity = 40, n_iter = 300)\n",
    "    tsne_results = tsne.fit_transform(final_arrayX)\n",
    "    \n",
    "    # Plotting\n",
    "    f, ax = plt.subplots(1)\n",
    "    \n",
    "    plt.scatter(tsne_results[:No,0], tsne_results[:No,1], c = colors[:No], alpha = 0.2, label = \"Original\")\n",
    "    plt.scatter(tsne_results[No:,0], tsne_results[No:,1], c = colors[No:], alpha = 0.2, label = \"Synthetic\")\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.title('t-SNE plot')\n",
    "    plt.xlabel('x-tsne')\n",
    "    plt.ylabel('y_tsne')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-hoc RNN one-step ahead predictor\n",
    "\n",
    "def predictive_score_metrics (dataX, dataX_hat):\n",
    "  \n",
    "    # Initialization on the Graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Basic Parameters\n",
    "    No = len(dataX)\n",
    "    data_dim = len(dataX[0][0,:])\n",
    "    \n",
    "    # Maximum seq length and each seq length\n",
    "    dataT = list()\n",
    "    Max_Seq_Len = 0\n",
    "    for i in range(No):\n",
    "        Max_Seq_Len = max(Max_Seq_Len, len(dataX[i][:,0]))\n",
    "        dataT.append(len(dataX[i][:,0]))\n",
    "     \n",
    "    # Network Parameters\n",
    "    hidden_dim = max(int(data_dim/2),1)\n",
    "    iterations = 5000\n",
    "    batch_size = 128\n",
    "    \n",
    "    ## input place holders\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, Max_Seq_Len-1, data_dim-1], name = \"myinput_x\")\n",
    "    T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")    \n",
    "    Y = tf.placeholder(tf.float32, [None, Max_Seq_Len-1, 1], name = \"myinput_y\")\n",
    "    \n",
    "    ## builde a RNN discriminator network \n",
    "    \n",
    "    def predictor (X, T):\n",
    "      \n",
    "        with tf.variable_scope(\"predictor\", reuse = tf.AUTO_REUSE) as vs:\n",
    "            \n",
    "            d_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'd_cell')\n",
    "                    \n",
    "            d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, X, dtype=tf.float32, sequence_length = T)\n",
    "                \n",
    "            Y_hat = tf.contrib.layers.fully_connected(d_outputs, 1, activation_fn=None) \n",
    "            \n",
    "            Y_hat_Final = tf.nn.sigmoid(Y_hat)\n",
    "            \n",
    "            d_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
    "    \n",
    "        return Y_hat_Final, d_vars\n",
    "    \n",
    "    ## Functions\n",
    "    # Variables\n",
    "    Y_pred, d_vars = predictor(X, T)\n",
    "        \n",
    "    # Loss for the predictor\n",
    "    D_loss = tf.losses.absolute_difference(Y, Y_pred)\n",
    "    \n",
    "    # optimizer\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
    "        \n",
    "    ## Sessions    \n",
    "\n",
    "    # Session start\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training using Synthetic dataset\n",
    "    for itt in range(iterations):\n",
    "          \n",
    "        # Batch setting\n",
    "        idx = np.random.permutation(len(dataX_hat))\n",
    "        train_idx = idx[:batch_size]     \n",
    "            \n",
    "        X_mb = list(dataX_hat[i][:-1,:(data_dim-1)] for i in train_idx)\n",
    "        T_mb = list(dataT[i]-1 for i in train_idx)\n",
    "        Y_mb = list(np.reshape(dataX_hat[i][1:,(data_dim-1)],[len(dataX_hat[i][1:,(data_dim-1)]),1]) for i in train_idx)        \n",
    "          \n",
    "        # Train discriminator\n",
    "        _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Y: Y_mb})            \n",
    "        \n",
    "        ## Checkpoints\n",
    "#        if itt % 500 == 0:\n",
    "#            print(\"[step: {}] loss - d loss: {}\".format(itt, np.sqrt(np.round(step_d_loss,4))))\n",
    "    \n",
    "    ## Use Original Dataset to test\n",
    "    \n",
    "    # Make Batch with Original Data\n",
    "    idx = np.random.permutation(len(dataX_hat))\n",
    "    train_idx = idx[:No]     \n",
    "    \n",
    "    X_mb = list(dataX[i][:-1,:(data_dim-1)] for i in train_idx)\n",
    "    T_mb = list(dataT[i]-1 for i in train_idx)\n",
    "    Y_mb = list(np.reshape(dataX[i][1:,(data_dim-1)], [len(dataX[i][1:,(data_dim-1)]),1]) for i in train_idx)\n",
    "    \n",
    "    # Predict Fugure\n",
    "    pred_Y_curr = sess.run(Y_pred, feed_dict={X: X_mb, T: T_mb})\n",
    "    \n",
    "    # Compute MAE\n",
    "    MAE_Temp = 0\n",
    "    for i in range(No):\n",
    "        MAE_Temp = MAE_Temp + mean_absolute_error(Y_mb[i], pred_Y_curr[i,:,:])\n",
    "    \n",
    "    MAE = MAE_Temp / No\n",
    "    \n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8afa2-6042-4e64-af22-e0734a3fddb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DISEASE TRAJECTORY STATE PREDICTIONS - MODEL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASSES:\n",
    "class HMM:\n",
    "    \n",
    "    def __init__(self, num_states):\n",
    "        \n",
    "        self.num_states = num_states\n",
    "        self.hmm_model  = hmm.GaussianHMM(n_components=num_states)\n",
    "        \n",
    "        \n",
    "    def fit(self, X_obs):\n",
    "        \n",
    "        X_obser = [[list(X_obs[u][k,:]) for k in range(len(X_obs[u]))] for u in range(len(X_obs))]\n",
    "        X_obs_  = np.concatenate(X_obser)\n",
    "\n",
    "        lengths = [len(X_obs[k]) for k in range(len(X_obs))]\n",
    "        \n",
    "        self.hmm_model.fit(X_obs_, lengths)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_new):\n",
    "        \n",
    "        preds       = np.concatenate([self.hmm_model.predict(X_new[k]) for k in range(len(X_new))])\n",
    "        predictions = [np.sum(np.concatenate([(self.hmm_model.transmat_[preds[k],u] * self.hmm_model.means_[u]).reshape((1,-1)) for u in range(self.num_states)]), axis=0).reshape((1,-1)) for k in range(len(preds))] \n",
    "        \n",
    "        return np.concatenate(predictions)\n",
    "        \n",
    "\n",
    "\n",
    "@tf_export(\"nn.rnn_cell.MultiRNNCell\")\n",
    "class MultiPhasedLSTMCell(MultiRNNCell):\n",
    "\n",
    "    def __init__(self, cells, state_is_tuple=True):\n",
    "        \n",
    "        super(MultiRNNCell, self).__init__()\n",
    "        \n",
    "        if not cells:\n",
    "            raise ValueError(\"Must specify at least one cell for MultiRNNCell.\")\n",
    "        if not nest.is_sequence(cells):\n",
    "            raise TypeError(\"cells must be a list or tuple, but saw: %s.\" % cells)\n",
    "\n",
    "        self._cells = cells\n",
    "        for cell_number, cell in enumerate(self._cells):\n",
    "        \n",
    "            # Add Checkpointable dependencies on these cells so their variables get\n",
    "            # saved with this object when using object-based saving.\n",
    "            if isinstance(cell, checkpointable.CheckpointableBase):\n",
    "                # TODO(allenl): Track down non-Checkpointable callers.\n",
    "                self._track_checkpointable(cell, name=\"cell-%d\" % (cell_number,))\n",
    "                self._state_is_tuple = state_is_tuple\n",
    "            \n",
    "            if not state_is_tuple:\n",
    "                if any(nest.is_sequence(c.state_size) for c in self._cells):\n",
    "                    raise ValueError(\"Some cells return tuples of states, but the flag \"\n",
    "                                     \"state_is_tuple is not set.  State sizes are: %s\"\n",
    "                                     % str([c.state_size for c in self._cells]))\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        if self._state_is_tuple:\n",
    "            return tuple(cell.state_size for cell in self._cells)\n",
    "        else:\n",
    "            return sum([cell.state_size for cell in self._cells])\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._cells[-1].output_size\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
    "            if self._state_is_tuple:\n",
    "                return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells)\n",
    "            else:\n",
    "                # We know here that state_size of each cell is not a tuple and\n",
    "                # presumably does not contain TensorArrays or anything else fancy\n",
    "                return super(MultiRNNCell, self).zero_state(batch_size, dtype)\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "        cur_state_pos = 0\n",
    "\n",
    "        times         = inputs[0]\n",
    "        cur_inp       = inputs[1]\n",
    "        \n",
    "        new_states    = []\n",
    "    \n",
    "        for i, cell in enumerate(self._cells):\n",
    "            with vs.variable_scope(\"cell_%d\" % i):\n",
    "                if self._state_is_tuple:\n",
    "                    if not nest.is_sequence(state):\n",
    "                        raise ValueError(\"Expected state to be a tuple of length %d, but received: %s\" %(len(self.state_size), state))\n",
    "                    cur_state = state[i]\n",
    "                else:\n",
    "                    cur_state = array_ops.slice(state, [0, cur_state_pos],\n",
    "                                      [-1, cell.state_size])\n",
    "                    cur_state_pos += cell.state_size\n",
    "                \n",
    "                cur_inp, new_state = cell((times, cur_inp), cur_state)\n",
    "                new_states.append(new_state)\n",
    "\n",
    "        new_states = (tuple(new_states) if self._state_is_tuple else array_ops.concat(new_states, 1))\n",
    "\n",
    "        return cur_inp, new_states\n",
    "\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def padd_data(X, padd_length):\n",
    "    \n",
    "    X_padded      = []\n",
    "    \n",
    "    for k in range(len(X)):\n",
    "        \n",
    "        if X[k].shape[0] < padd_length:\n",
    "            \n",
    "            if len(X[k].shape) > 1:\n",
    "                X_padded.append(np.array(np.vstack((np.array(X[k]), \n",
    "                                                    np.zeros((padd_length-X[k].shape[0],X[k].shape[1]))))))\n",
    "            else:\n",
    "                X_padded.append(np.array(np.vstack((np.array(X[k]).reshape((-1,1)),\n",
    "                                                    np.zeros((padd_length-X[k].shape[0],1))))))\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            if len(X[k].shape) > 1:\n",
    "                X_padded.append(np.array(X[k]))\n",
    "            else:\n",
    "                X_padded.append(np.array(X[k]).reshape((-1,1)))\n",
    "  \n",
    "\n",
    "    X_padded      = np.array(X_padded)\n",
    "\n",
    "    return X_padded\n",
    "\n",
    "\n",
    "def flatten_sequences_to_numpy(sequence_list):\n",
    "    \n",
    "    seqLists   = [list(itertools.chain.from_iterable(sequence_list[k].tolist())) for k in range(len(sequence_list))]\n",
    "    flat_seqs  = np.array(list(itertools.chain.from_iterable(seqLists)))\n",
    "    \n",
    "    return flat_seqs\n",
    "\n",
    "\n",
    "def state_to_array(state_index, number_of_states):\n",
    "    \n",
    "    state_array = np.zeros(number_of_states)\n",
    "    state_array[state_index] = 1\n",
    "    \n",
    "    return state_array\n",
    "    \n",
    "\n",
    "def get_transitions(state_array, num_states):\n",
    "    \n",
    "    trans_matrix = np.zeros((num_states, num_states))\n",
    "    each_state   = [np.sum((state_array==k)*1) for k in range(num_states)]\n",
    "    \n",
    "    for k in range(num_states):\n",
    "        \n",
    "        where_states       = np.where(state_array==k)[0]\n",
    "        where_states_      = where_states[where_states < len(state_array) - 1]\n",
    "        \n",
    "        after_states       = [state_array[where_states_[k] + 1] for k in range(len(where_states_))]\n",
    "        trans_matrix[k, :] = np.array([(np.where(np.array(after_states)==k))[0].shape[0] for k in range(num_states)])\n",
    "        \n",
    "    return trans_matrix, each_state\n",
    "\n",
    "\n",
    "class attentive_state_space_model:\n",
    "    \n",
    "    '''\n",
    "    Class for the \"Attentive state space model\" implementation. Based on the paper: \n",
    "    \"Attentive state space model for disease progression\" by Ahmed M. Alaa and Mihaela van der Schaar.\n",
    "    \n",
    "    ** Key arguments **\n",
    "    \n",
    "    :param maximum_seq_length: Maximum allowable length for any trajectory in the training data. \n",
    "    :param input_dim: Dimensionality of the observations (emissions).\n",
    "    :param num_states: Cardinality of the state space.\n",
    "    :param inference_network: Configuration of the inference network. Default is: 'Seq2SeqAttention'.\n",
    "    :param rnn_type: Type of RNN cells to use in the inference network. Default is 'LSTM'.\n",
    "    :param unsupervised: Boolean for whether the model is supervised or unsupervised. Default is True. \n",
    "                         Supervised is NOT IMPLEMENTED.\n",
    "    :param generative: Boolean for whether to enable sampling from the model. \n",
    "    :param irregular: Whether the trajectories are in continuous time. NOT IMPLEMENTED.\n",
    "    :param multitask: Boolean for whether multi-task output layers are used in inference network. NOT IMPLEMENTED\n",
    "    :param num_iterations: Number of iterations for the stochastic variational inference algorithm.\n",
    "    :param num_epochs: Number of epochs for the stochastic variational inference algorithm.\n",
    "    :param batch_size: Size of the batch subsampled from the training data.\n",
    "    :param learning_rate: Learning rate for the ADAM optimizer. (TO DO: enable selection of the optimizer)\n",
    "    :param num_rnn_hidden: Size of the RNN layers used in the inference network.\n",
    "    :param num_rnn_layers: Number of RNN layers used in the inference network.\n",
    "    :param dropout_keep_prob: Dropout probability. Default is None.\n",
    "    :param num_out_hidden: Size of output layer in inference network.\n",
    "    :param num_out_layers: Size of output layer in inference network.\n",
    "    ** Key attributes **\n",
    "    \n",
    "    After fitting the model, the key model parameters are stored in the following attributes:\n",
    "    \n",
    "    :attr states_mean: Mean of each observation in each of the num_states states.\n",
    "    :attr states_covars: Covariance matrices of observations.\n",
    "    :attr transition_matrix: Baseline Markov transition matrix for the attentive state space.\n",
    "    :attr intial probabilities: Initial distribution of states averaged accross all trajectories in training data.\n",
    "    \n",
    "    ** Key methods **\n",
    "    \n",
    "    Three key methods are implemented in the API:\n",
    "    \n",
    "    :method fit: Takes a list of observations and fits an attentive state space model in an unsupervised fashion.\n",
    "    :method predict: Takes a new observation and returns three variables:\n",
    "                     - Prediction of the next state at every time step.\n",
    "                     - Expected observation at the next time tep.\n",
    "                     - List of attention weights assigned to previous states at every time step.\n",
    "    :method sample: This method samples synthetic trajectories from the model.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 maximum_seq_length, \n",
    "                 input_dim, \n",
    "                 num_states=3,\n",
    "                 inference_network='Seq2SeqAttention', \n",
    "                 rnn_type='LSTM',\n",
    "                 unsupervised=True,\n",
    "                 generative=True,\n",
    "                 irregular=False,\n",
    "                 multitask=False,\n",
    "                 input_name=\"Input\", \n",
    "                 output_name=\"Output\",\n",
    "                 model_name=\"SeqModel\",\n",
    "                 num_iterations=50, \n",
    "                 num_epochs=10, \n",
    "                 batch_size=100, \n",
    "                 learning_rate=5*1e-4, \n",
    "                 num_rnn_hidden=100, \n",
    "                 num_rnn_layers=1,\n",
    "                 dropout_keep_prob=None,\n",
    "                 num_out_hidden=100, \n",
    "                 num_out_layers=1,\n",
    "                 verbosity=True,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        \n",
    "        # Set all model variables\n",
    "\n",
    "        self.maximum_seq_length = maximum_seq_length \n",
    "        self.input_dim          = input_dim\n",
    "        self.num_states         = num_states\n",
    "        self.inference_network  = inference_network\n",
    "        self.rnn_type           = rnn_type\n",
    "        self.unsupervised       = unsupervised\n",
    "        self.generative         = generative\n",
    "        self.irregular          = irregular\n",
    "        self.multitask          = multitask\n",
    "        self.input_name         = input_name \n",
    "        self.output_name        = output_name \n",
    "        self.model_name         = model_name\n",
    "        self.num_iterations     = num_iterations\n",
    "        self.num_epochs         = num_epochs\n",
    "        self.batch_size         = batch_size\n",
    "        self.learning_rate      = learning_rate\n",
    "        self.num_rnn_hidden     = num_rnn_hidden\n",
    "        self.num_rnn_layers     = num_rnn_layers\n",
    "        self.dropout_keep_prob  = dropout_keep_prob\n",
    "        self.num_out_hidden     = num_out_hidden\n",
    "        self.num_out_layers     = num_out_layers\n",
    "        self.verbosity          = verbosity\n",
    "        \n",
    "        \n",
    "        self.build_attentive_inference_network()\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.build_attentive_inference_graph()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def build_attentive_inference_network(self):\n",
    "        \n",
    "        # replace this with dictionary style indexing\n",
    "        \n",
    "        model_options_names     = ['RNN','LSTM','GRU','PhasedLSTM']\n",
    "        \n",
    "        optimizer_options_names = []\n",
    "        \n",
    "        \n",
    "        model_options   = [BasicRNNCell(self.num_rnn_hidden), \n",
    "                           rnn_cell.LSTMCell(self.num_rnn_hidden), \n",
    "                           rnn_cell.GRUCell(self.num_rnn_hidden), \n",
    "                           PhasedLSTMCell(self.num_rnn_hidden)]\n",
    "        \n",
    "        self._rnn_model = model_options[np.where(np.array(model_options_names)==self.rnn_type)[0][0]]\n",
    "        \n",
    "        if self.dropout_keep_prob is not None:\n",
    "            \n",
    "            self._rnn_model = tf.nn.rnn_cell.DropoutWrapper(self._rnn_model, output_keep_prob=self.dropout_keep_prob)\n",
    "        \n",
    "        self._Losses = []\n",
    "        \n",
    "\n",
    "    def build_attentive_inference_graph(self):\n",
    "        \n",
    "        self.observation = tf.placeholder(tf.float32, \n",
    "                                     [None, self.maximum_seq_length, self.input_dim], \n",
    "                                     name=self.input_name)\n",
    "            \n",
    "        self.state_guess = tf.placeholder(tf.float32, \n",
    "                                     [None, self.maximum_seq_length, self.num_states]) \n",
    "        \n",
    "        if self.irregular:\n",
    "            \n",
    "            self.times      = tf.placeholder(tf.float32, [None, self.maximum_seq_length, 1])\n",
    "            self.rnn_input  = (self.times, self.observation)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            self.rnn_input  = self.observation\n",
    "\n",
    "            \n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        \n",
    "        used   = tf.sign(tf.reduce_max(tf.abs(self.observation), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        \n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def forward(self):\n",
    "        \n",
    "        self.attentive_inference_network_inputs()\n",
    "        \n",
    "        # Recurrent network.   \n",
    "        if self.inference_network != 'Seq2SeqAttention': \n",
    "            \n",
    "            rnn_output, _  = rnn.dynamic_rnn(self._rnn_model, \n",
    "                                             self.rnn_input_, \n",
    "                                             dtype=tf.float32, \n",
    "                                             sequence_length=self.length_,)\n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "                tf.nn.rnn_cell = tf.contrib.rnn\n",
    "                tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "\n",
    "                if self.verbosity:\n",
    "                    \n",
    "                    print(\"TensorFlow version : >= 1.0\")\n",
    "            \n",
    "            except: \n",
    "            \n",
    "                print(\"TensorFlow version : 0.12\")\n",
    "            \n",
    "            if self.verbosity:\n",
    "                \n",
    "                print(\"---------------------------\")\n",
    "            \n",
    "            self.enc_inp    = [self.rnn_input_[:, t, :] for t in range(self.maximum_seq_length)]\n",
    "\n",
    "            self.dec_output = [tf.placeholder(tf.float32, shape=(None, 1), \n",
    "                                              name=\"dec_output_\".format(t)) for t in range(self.maximum_seq_length)]\n",
    "\n",
    "            self.dec_inp    = [tf.zeros_like(self.enc_inp[0], dtype=np.float32, name=\"GO\")] + self.enc_inp[:-1] \n",
    "\n",
    "            self.cells = []\n",
    "    \n",
    "            for i in range(self.num_rnn_layers):\n",
    "                \n",
    "                with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                    \n",
    "                    self.cells.append(tf.nn.rnn_cell.LSTMCell(self.num_rnn_hidden))\n",
    "            \n",
    "            \n",
    "            self.cell  = tf.nn.rnn_cell.MultiRNNCell(self.cells)\n",
    "            self.dec_outputs, self.dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(self.enc_inp, self.dec_inp, self.cell) \n",
    "            \n",
    "            self.weight_dec, self.bias_dec = self._weight_and_bias(self.num_rnn_hidden, 1, [\"w_dec\", \"b_dec\"])\n",
    "            \n",
    "            self.seq2seq_attn = [(tf.matmul(i, self.weight_dec) + self.bias_dec) for i in self.dec_outputs]\n",
    "            self.seq2seq_attn = tf.nn.softmax(tf.reshape(tf.stack(self.seq2seq_attn), \n",
    "                                                         [-1, self.maximum_seq_length, 1]), axis=1)\n",
    "            \n",
    "        \n",
    "        # Softmax layer.\n",
    "        self.combiner_func_weight, self.combiner_func_bias = self._weight_and_bias(self.input_dim, \n",
    "                                                                                   self.num_out_hidden, \n",
    "                                                                                   [\"w_0\", \"b_0\"])\n",
    "        \n",
    "        self.weight, self.bias     = self._weight_and_bias(self.num_out_hidden, \n",
    "                                                           self.num_states, \n",
    "                                                           [\"w\", \"b\"])\n",
    "            \n",
    "        # Flatten to apply same weights to all time steps.\n",
    "        \n",
    "        if self.inference_network not in ['RETAIN', 'Seq2SeqAttention']: \n",
    "            \n",
    "            rnn_output  = tf.reshape(rnn_output, [-1, self.num_out_hidden])\n",
    "            \n",
    "            forward     = tf.nn.softmax(tf.matmul(rnn_output, self.weight) + self.bias)\n",
    "        \n",
    "        elif self.inference_network == 'RETAIN':\n",
    "            \n",
    "            self.weight_a, self.bias_a = self._weight_and_bias(self.num_out_hidden, 1, [\"w_a\", \"b_a\"])\n",
    "            \n",
    "            rnn_output      = tf.reshape(rnn_output, [-1, self.num_out_hidden])\n",
    "            \n",
    "            self.attention  = tf.nn.softmax(tf.reshape(tf.matmul(rnn_output, self.weight_a) + self.bias_a, \n",
    "                                                       [-1, self.maximum_seq_length, 1]), axis=1)\n",
    "            \n",
    "\n",
    "            attn_mask       = tf.expand_dims(tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2)), axis=2)\n",
    "            masked_attn     = tf.multiply(attn_mask, self.attention)\n",
    "            attn_norms      = tf.expand_dims(tf.tile(tf.reduce_sum(masked_attn, axis=1), [1, self.maximum_seq_length]), axis=2)\n",
    "            self.attention  = masked_attn/attn_norms\n",
    "            self.attention_ = tf.tile(self.attention, [1, 1, self.input_dim])\n",
    "            self.context    = tf.reduce_sum(tf.multiply(self.attention_, self.rnn_input_), reduction_indices=1)\n",
    "            \n",
    "            context_layer   = tf.matmul(self.context, self.combiner_func_weight ) + self.combiner_func_bias\n",
    "            forward         = tf.nn.softmax(tf.matmul(context_layer, self.weight) + self.bias)\n",
    "        \n",
    "        elif self.inference_network == 'Seq2SeqAttention':\n",
    "            \n",
    "            self.attention  = self.seq2seq_attn\n",
    "            \n",
    "            attn_mask       = tf.expand_dims(tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2)), axis=2)            \n",
    "            masked_attn     = tf.multiply(attn_mask, self.attention)\n",
    "            attn_norms      = tf.expand_dims(tf.tile(tf.reduce_sum(masked_attn, axis=1), [1, self.maximum_seq_length]), axis=2)\n",
    "            self.attention  = masked_attn/attn_norms\n",
    "            self.attention_ = tf.tile(self.attention, [1, 1, self.input_dim])\n",
    "            self.context    = tf.reduce_sum(tf.multiply(self.attention_, self.rnn_input_), reduction_indices=1)\n",
    "            context_layer   = tf.matmul(self.context, self.combiner_func_weight ) + self.combiner_func_bias\n",
    "            forward         = tf.nn.softmax(tf.matmul(context_layer, self.weight) + self.bias)\n",
    "\n",
    "        forward         = tf.reshape(forward, [-1, self.maximum_seq_length, self.num_states])\n",
    "        \n",
    "        self.predicted  = forward\n",
    "        self.predicted  = tf.identity(self.predicted, name=self.output_name)\n",
    "        \n",
    "        return forward\n",
    "\n",
    "    \n",
    "    def attentive_inference_network_inputs(self):\n",
    "        \n",
    "        if self.inference_network in ['RETAIN', 'Seq2SeqAttention']: \n",
    "            \n",
    "            self.num_samples = tf.shape(self.observation)[0] \n",
    "             \n",
    "            Lengths_         = np.repeat(self.length, self.maximum_seq_length)\n",
    "            \n",
    "            conv_data        = tf.reshape(tf.tile(self.observation, [1, self.maximum_seq_length, 1]), \n",
    "                                          [self.maximum_seq_length * self.num_samples, \n",
    "                                           self.maximum_seq_length, self.input_dim])\n",
    "            \n",
    "            conv_mask_       = tf.ones([self.maximum_seq_length, self.maximum_seq_length], tf.float32)\n",
    "            \n",
    "            conv_mask        = tf.tile(tf.expand_dims(tf.tile(tf.matrix_band_part(conv_mask_, -1, 0), \n",
    "                                                              [self.num_samples, 1]), 2), \n",
    "                                                              [1, 1, self.input_dim])\n",
    "            \n",
    "            masked_data   = tf.multiply(conv_data, conv_mask)\n",
    "            \n",
    "            Seq_lengths_  = tf.tile(tf.range(1, self.maximum_seq_length + 1, 1), [self.num_samples])\n",
    "            \n",
    "            if self.inference_network == 'RETAIN':\n",
    "            \n",
    "                self.rnn_input_  = tf.reverse_sequence(masked_data, batch_axis=0, seq_dim=1, \n",
    "                                                       seq_lengths=Seq_lengths_, seq_axis=None)\n",
    "            else:\n",
    "                \n",
    "                self.rnn_input_  = masked_data\n",
    "                \n",
    "            \n",
    "            used         = tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2))\n",
    "            length       = tf.reduce_sum(used, reduction_indices=1)\n",
    "            self.length_ = tf.cast(length, tf.int32)\n",
    "\n",
    "            \n",
    "        else:    \n",
    "            \n",
    "            self.rnn_input_ = self.rnn_input\n",
    "            self.length_    = self.length\n",
    "\n",
    "            \n",
    "    @lazy_property\n",
    "    def ELBO(self):\n",
    "\n",
    "        mask                = tf.sign(tf.reduce_max(tf.abs(self.observation), reduction_indices=2))\n",
    "        flat_mask           = tf.reshape(mask, [-1,1])\n",
    "        \n",
    "        flat_state_guess    = tf.reshape(self.state_guess, [-1, self.num_states])      \n",
    "        flat_forward        = tf.reshape(self.forward, [-1, self.num_states])\n",
    "\n",
    "        likelihood_loss     = tf.reduce_sum(-1*(flat_state_guess * tf.log(flat_forward)))\n",
    "        \n",
    "        self.mask           = mask\n",
    "        \n",
    "        # Average over actual sequence lengths. << Did I forget masking padded ELBOs? >>\n",
    "        likelihood_loss     /= tf.reduce_sum(tf.cast(self.length, tf.float32),reduction_indices=0)\n",
    "        \n",
    "        return likelihood_loss        \n",
    "            \n",
    "        \n",
    "    def initialize_hidden_states(self, X):\n",
    "        \n",
    "        self.init_states = GaussianMixture(n_components=self.num_states, \n",
    "                                           covariance_type='full')\n",
    "        # print('Dims: ', self.input_dim)\n",
    "        # print('X: ', X[0:2])\n",
    "        self.init_states.fit(np.concatenate(X).reshape((-1, self.input_dim))) \n",
    "        \n",
    "        \n",
    "    def get_likelihood(self, X, pred):\n",
    "        \n",
    "        likelihoods_ = []\n",
    " \n",
    "        XX    = X.reshape((-1, self.input_dim))\n",
    "        \n",
    "        # lks_  = np.array([multivariate_normal.logpdf(XX, self.state_means[k], self.state_covars[k]).reshape((-1,1))*pred[:,k].reshape((-1,1)) for k in range(self.num_states)])\n",
    "        # TODO: to get to work with more features, added allow_singular=True, come back to see if better fix\n",
    "        lks_  = np.array([multivariate_normal.logpdf(XX, self.state_means[k], self.state_covars[k], allow_singular=True).reshape((-1,1))*pred[:,k].reshape((-1,1)) for k in range(self.num_states)])\n",
    "\n",
    "        likelihoods_  = lks_\n",
    "        \n",
    "        return np.mean(likelihoods_[np.isfinite(likelihoods_)])\n",
    "    \n",
    "    \n",
    "    def sample_posterior_states(self, q_posterior):\n",
    "        \n",
    "        sampled_list = [state_to_array(np.random.choice(self.num_states, 1, p=q_posterior[k,:])[0], self.num_states) for k in range(q_posterior.shape[0])]\n",
    "        self.state_trajectories = np.array(sampled_list)\n",
    "    \n",
    "    def fit(self, X, T=None):\n",
    "        \n",
    "        self.state_trajectories_ = []\n",
    "        \n",
    "        self.initialize_hidden_states(X)\n",
    "\n",
    "        state_inferences_init  = [np.argmax(self.init_states.predict_proba(X[k]), axis=1) for k in range(len(X))]\n",
    "        self.all_states        = state_inferences_init\n",
    "\n",
    "        for v in range(len(state_inferences_init)):\n",
    "            \n",
    "            state_list = [state_to_array(state_inferences_init[v][k], self.num_states) for k in range(len(state_inferences_init[v]))]\n",
    "            delayed_traject = np.vstack((np.array(state_list)[1:, :], np.array(state_list)[-1, :]))\n",
    "            \n",
    "            self.state_trajectories_.append(delayed_traject)\n",
    "            \n",
    "            \n",
    "        self.normalizer   = StandardScaler()\n",
    "        self.normalizer.fit(np.concatenate(X))\n",
    "\n",
    "        self.X_normalized  = []\n",
    "\n",
    "        for k in range(len(X)):\n",
    "            \n",
    "            self.X_normalized.append(self.normalizer.transform(X[k])) \n",
    "    \n",
    "\n",
    "        self.stochastic_variational_inference(self.X_normalized)\n",
    "\n",
    "        \n",
    "    def stochastic_variational_inference(self, X, T=None):\n",
    "        \n",
    "        X_, state_update = padd_data(X, self.maximum_seq_length), padd_data(self.state_trajectories_, self.maximum_seq_length)\n",
    "        \n",
    "        if T is not None:\n",
    "            T_   = padd_data(T, self.maximum_seq_length)\n",
    "            \n",
    "            \n",
    "        # Baseline transition matrix\n",
    "\n",
    "        initial_states = np.array([self.all_states[k][0] for k in range(len(self.all_states))])\n",
    "        init_probs     = [np.where(initial_states==k)[0].shape[0] / len(initial_states) for k in range(self.num_states)]\n",
    "\n",
    "        transits   = np.zeros((self.num_states, self.num_states))\n",
    "        each_state = np.zeros(self.num_states)\n",
    "\n",
    "        for _ in range(len(self.all_states)):\n",
    "    \n",
    "            new_trans, new_each_state = get_transitions(self.all_states[_], self.num_states)\n",
    "    \n",
    "            transits   += new_trans\n",
    "            each_state += new_each_state\n",
    "    \n",
    "        for _ in range(self.num_states):\n",
    "    \n",
    "            transits[_, :] = transits[_, :] / each_state[_]\n",
    "            transits[_, :] = transits[_, :] / np.sum(transits[_, :])\n",
    "    \n",
    "        self.initial_probabilities = np.array(init_probs)\n",
    "        self.transition_matrix     = np.array(transits)\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # Observational distribution\n",
    "        # -----------------------------------------------------------\n",
    "        \n",
    "        self.state_means  = self.init_states.means_\n",
    "        self.state_covars = self.init_states.covariances_    \n",
    "        \n",
    "        \n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        opt      = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.ELBO)\n",
    "        init     = tf.global_variables_initializer()\n",
    "        \n",
    "        sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "                \n",
    "            for _ in range(self.num_iterations):\n",
    "                \n",
    "                batch_samples = np.random.choice(list(range(X_.shape[0])), size=self.batch_size, replace=False)\n",
    "                # print(batch_samples[0:3])\n",
    "                # print('batch samples: ', batch_samples[0:10])\n",
    "                # print(X_)\n",
    "                # print('x shape 1: ', X_.shape)\n",
    "                # # X_ = X_.reshape(X_.shape[0],3)\n",
    "                # X_ = X_.reshape(int(X_.shape[0]/3),3)\n",
    "                # print('x shape 2: ', X_.shape)\n",
    "                \n",
    "                batch_train   = X_[batch_samples,:,:]\n",
    "                # batch_train   = X_[batch_samples , : , : ]\n",
    "                batch_states  = state_update[batch_samples,:,:]\n",
    "                \n",
    "                train_dict    = {self.observation : batch_train,\n",
    "                                 self.state_guess : batch_states}\n",
    "                \n",
    "                batch_preds   = sess.run(self.forward, feed_dict=train_dict)\n",
    "                \n",
    "                # sample and update posterior states\n",
    "                self.sample_posterior_states(batch_preds.reshape((-1, self.num_states)))\n",
    "\n",
    "                sess.run(opt, feed_dict=train_dict)\n",
    "                \n",
    "                Loss = sess.run(self.ELBO, feed_dict=train_dict)\n",
    "                \n",
    "                log_likelihood_ = np.array([self.get_likelihood(batch_train[k,:,:], batch_preds[k,:,:]) for k in range(batch_train.shape[0])]) \n",
    "                log_likelihood_ = np.sum(log_likelihood_)/self.batch_size\n",
    "                \n",
    "                self._Losses.append(log_likelihood_)\n",
    " \n",
    "                # Verbosity function\n",
    "    \n",
    "                if self.verbosity:\n",
    "            \n",
    "                    print('Epoch %d \\t----- \\tBatch %d \\t----- \\tLog-Likelihood %10.6e' % (epoch, _, log_likelihood_))\n",
    "                \n",
    "        \n",
    "        # Save model\n",
    "        saver.save(sess, \"./mlaimRNN_model\") \n",
    "        \n",
    "        if os.path.exists(\"attentive_state_space\"):\n",
    "            \n",
    "            shutil.rmtree(\"attentive_state_space\")\n",
    "        \n",
    "        tf.saved_model.simple_save(sess, export_dir='attentive_state_space', inputs={\"myInput\": self.observation}, \n",
    "                                   outputs={\"myOutput\": self.predicted})    \n",
    "        \n",
    "           \n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_normalized  = []\n",
    "\n",
    "        for k in range(len(X)):\n",
    "            \n",
    "            X_normalized.append(self.normalizer.transform(X[k])) \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            saver           = tf.train.import_meta_graph(\"mlaimRNN_model.meta\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "            \n",
    "            preds_lengths   = [len(X_normalized[k]) for k in range(len(X_normalized))]\n",
    "            \n",
    "            X_pred          = padd_data(X_normalized, padd_length=self.maximum_seq_length)\n",
    "            pred_dict       = {self.observation  : X_pred}\n",
    "            \n",
    "            prediction_     = sess.run(self.forward, pred_dict).reshape([-1, self.maximum_seq_length, self.num_states])         \n",
    "\n",
    "            preds_          = []\n",
    "            obs_            = []\n",
    "            \n",
    "            for k in range(len(X)):\n",
    "                \n",
    "                preds_.append(prediction_[k, 0 : preds_lengths[k]])\n",
    "                obs_.append(self.get_observations(preds_[-1]))\n",
    "\n",
    "            if self.inference_network in ['RETAIN', 'Seq2SeqAttention']:\n",
    "                \n",
    "                \n",
    "                attn_                  = sess.run(self.attention, pred_dict) \n",
    "                attn_per_patient       = [attn_[u * self.maximum_seq_length : u * self.maximum_seq_length + self.maximum_seq_length, :, :] for u in range(len(X))]\n",
    "                attn_lists_per_patient = [[attn_per_patient[u][k, 0 : k + 1, :] for k in range(self.maximum_seq_length)] for u in range(len(X))]\n",
    "                \n",
    "                all_preds_             = (preds_, obs_, attn_lists_per_patient)    \n",
    "\n",
    "            else:\n",
    "                \n",
    "                all_preds_             = (preds_, obs_) \n",
    "\n",
    "                \n",
    "        return all_preds_ \n",
    "    \n",
    "    \n",
    "    def get_observations(self, preds):\n",
    "        \n",
    "        pred_obs     = []\n",
    "    \n",
    "        for v in range(preds.shape[0]):\n",
    "        \n",
    "            observations = np.zeros(self.input_dim)\n",
    "    \n",
    "            for k in range(self.num_states):\n",
    "            \n",
    "                observations += self.state_means[k] * preds[v, k] \n",
    "    \n",
    "            pred_obs.append(observations) \n",
    "    \n",
    "    \n",
    "        return np.array(pred_obs)\n",
    "    \n",
    "    \n",
    "    def sample(self, trajectory_length=5):\n",
    "        \n",
    "        initial_state    = np.random.choice(self.num_states, 1, \n",
    "                                            p=self.initial_probabilities)[0]\n",
    "    \n",
    "        State_trajectory      = [initial_state]\n",
    "        first_observation     = np.random.multivariate_normal(self.state_means[initial_state], \n",
    "                                                              self.state_covars[initial_state]) \n",
    "    \n",
    "        Obervation_trajectory = [first_observation.reshape((1,-1))]\n",
    "    \n",
    "        for _ in range(trajectory_length):\n",
    "        \n",
    "            next_state_pred  = self.predict(Obervation_trajectory)[0][0][0]\n",
    "            next_state       = np.random.choice(self.num_states, 1, p=next_state_pred)[0]\n",
    "        \n",
    "            State_trajectory.append(next_state)\n",
    "        \n",
    "            next_observation = np.random.multivariate_normal(self.state_means[next_state], \n",
    "                                                         self.state_covars[next_state]).reshape((1,-1))\n",
    "        \n",
    "        \n",
    "            Obervation_trajectory[0] = np.vstack((Obervation_trajectory[0], next_observation))\n",
    "        \n",
    "    \n",
    "        return State_trajectory, Obervation_trajectory   \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size, wnames):\n",
    "    \n",
    "        weight = tf.get_variable(wnames[0], \n",
    "                                 shape=[in_size, out_size], \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        bias   = tf.get_variable(wnames[1], \n",
    "                                 shape=[out_size], \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        return weight, bias\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class SeqModel:\n",
    "    \n",
    "    '''\n",
    "    Parent class for all RNN models.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 maximum_seq_length, \n",
    "                 input_dim, \n",
    "                 output_dim=1,\n",
    "                 model_type='RNN',\n",
    "                 rnn_type='RNN',\n",
    "                 latent=False,\n",
    "                 generative=False,\n",
    "                 irregular=False,\n",
    "                 multitask=False,\n",
    "                 prediction_mode='Sequence_labeling',\n",
    "                 input_name=\"Input\", \n",
    "                 output_name=\"Output\",\n",
    "                 model_name=\"SeqModel\",\n",
    "                 num_iterations=20, \n",
    "                 num_epochs=10, \n",
    "                 batch_size=100, \n",
    "                 learning_rate=0.0005, \n",
    "                 num_rnn_hidden=200, \n",
    "                 num_rnn_layers=1,\n",
    "                 dropout_keep_prob=None,\n",
    "                 num_out_hidden=200, \n",
    "                 num_out_layers=1,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        \n",
    "        # Set all model variables\n",
    "\n",
    "        self.maximum_seq_length = maximum_seq_length \n",
    "        self.input_dim          = input_dim\n",
    "        self.output_dim         = output_dim\n",
    "        self.model_type         = model_type\n",
    "        self.rnn_type           = rnn_type\n",
    "        self.latent             = latent\n",
    "        self.generative         = generative\n",
    "        self.irregular          = irregular\n",
    "        self.multitask          = multitask\n",
    "        self.prediction_mode    = prediction_mode\n",
    "        self.input_name         = input_name \n",
    "        self.output_name        = output_name \n",
    "        self.model_name         = model_name\n",
    "        self.num_iterations     = num_iterations\n",
    "        self.num_epochs         = num_epochs\n",
    "        self.batch_size         = batch_size\n",
    "        self.learning_rate      = learning_rate\n",
    "        self.num_rnn_hidden     = num_rnn_hidden\n",
    "        self.num_rnn_layers     = num_rnn_layers\n",
    "        self.dropout_keep_prob  = dropout_keep_prob\n",
    "        self.num_out_hidden     = num_out_hidden\n",
    "        self.num_out_layers     = num_out_layers\n",
    "        \n",
    "        \n",
    "        self.build_rnn_model()\n",
    "        tf.reset_default_graph()\n",
    "        self.build_rnn_graph()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def build_rnn_model(self):\n",
    "        \n",
    "        # replace this with dictionary style indexing\n",
    "        \n",
    "        model_options_names     = ['RNN','LSTM','GRU','PhasedLSTM']\n",
    "        \n",
    "        optimizer_options_names = []\n",
    "        \n",
    "        \n",
    "        model_options   = [BasicRNNCell(self.num_rnn_hidden), rnn_cell.LSTMCell(self.num_rnn_hidden), \n",
    "                                   rnn_cell.GRUCell(self.num_rnn_hidden), PhasedLSTMCell(self.num_rnn_hidden)]\n",
    "        \n",
    "        self._rnn_model = model_options[np.where(np.array(model_options_names)==self.rnn_type)[0][0]]\n",
    "        \n",
    "        if self.dropout_keep_prob is not None:\n",
    "            \n",
    "            self._rnn_model = tf.nn.rnn_cell.DropoutWrapper(self._rnn_model, output_keep_prob=self.dropout_keep_prob)\n",
    "        \n",
    "        self._Losses = []\n",
    "        \n",
    "\n",
    "    def build_rnn_graph(self):\n",
    "        \n",
    "        self.data   = tf.placeholder(tf.float32, \n",
    "                                     [None, self.maximum_seq_length, self.input_dim], \n",
    "                                     name=self.input_name)\n",
    "            \n",
    "        self.target = tf.placeholder(tf.float32, \n",
    "                                     [None, self.maximum_seq_length, self.output_dim]) \n",
    "        \n",
    "        if self.irregular:\n",
    "            \n",
    "            self.times      = tf.placeholder(tf.float32, [None, self.maximum_seq_length, 1])\n",
    "            self.rnn_input  = (self.times, self.data)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            self.rnn_input  = self.data \n",
    "\n",
    "            \n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        \n",
    "        used   = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        \n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        \n",
    "        self.process_rnn_inputs()\n",
    "        \n",
    "        # Recurrent network.   \n",
    "        if self.model_type != 'Seq2SeqAttention': \n",
    "            \n",
    "            rnn_output, _  = rnn.dynamic_rnn(self._rnn_model, \n",
    "                                             self.rnn_input_, \n",
    "                                             dtype=tf.float32, \n",
    "                                             sequence_length=self.length_,)\n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "                tf.nn.rnn_cell = tf.contrib.rnn\n",
    "                tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "                print(\"TensorFlow version : >= 1.0\")\n",
    "            \n",
    "            except: \n",
    "            \n",
    "                print(\"TensorFlow version : 0.12\")\n",
    "            \n",
    "            self.enc_inp    = [self.rnn_input_[:, t, :] for t in range(self.maximum_seq_length)]\n",
    "\n",
    "            self.dec_output = [tf.placeholder(tf.float32, shape=(None, 1), \n",
    "                                              name=\"dec_output_\".format(t)) for t in range(self.maximum_seq_length)]\n",
    "\n",
    "            self.dec_inp    = [tf.zeros_like(self.enc_inp[0], dtype=np.float32, name=\"GO\")] + self.enc_inp[:-1] \n",
    "\n",
    "            self.cells = []\n",
    "    \n",
    "            for i in range(self.num_rnn_layers):\n",
    "                \n",
    "                with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                    \n",
    "                    self.cells.append(tf.nn.rnn_cell.GRUCell(self.num_rnn_hidden))\n",
    "            \n",
    "            \n",
    "            self.cell  = tf.nn.rnn_cell.MultiRNNCell(self.cells)\n",
    "            self.dec_outputs, self.dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(self.enc_inp, self.dec_inp, self.cell) \n",
    "            \n",
    "            self.weight_dec, self.bias_dec = self._weight_and_bias(self.num_rnn_hidden, self.output_dim, [\"w_dec\", \"b_dec\"])\n",
    "            \n",
    "            self.seq2seq_attn = [(tf.matmul(i, self.weight_dec) + self.bias_dec) for i in self.dec_outputs]\n",
    "            self.seq2seq_attn = tf.nn.softmax(tf.reshape(tf.stack(self.seq2seq_attn), \n",
    "                                                         [-1, self.maximum_seq_length, 1]), axis=1)\n",
    "            \n",
    "        \n",
    "        # Softmax layer.\n",
    "        self.weight_0, self.bias_0 = self._weight_and_bias(self.input_dim, \n",
    "                                                           self.num_out_hidden, \n",
    "                                                           [\"w_0\", \"b_0\"])\n",
    "        \n",
    "        self.weight, self.bias     = self._weight_and_bias(self.num_out_hidden, \n",
    "                                                           self.output_dim, \n",
    "                                                           [\"w\", \"b\"])\n",
    "            \n",
    "        # Flatten to apply same weights to all time steps.\n",
    "        \n",
    "        if self.model_type not in ['RETAIN', 'Seq2SeqAttention']: \n",
    "            \n",
    "            rnn_output  = tf.reshape(rnn_output, [-1, self.num_out_hidden])\n",
    "            \n",
    "            prediction  = tf.nn.softmax(tf.matmul(rnn_output, self.weight) + self.bias)\n",
    "        \n",
    "        elif self.model_type == 'RETAIN':\n",
    "            \n",
    "            self.weight_a, self.bias_a = self._weight_and_bias(self.num_out_hidden, 1, [\"w_a\", \"b_a\"])\n",
    "            \n",
    "            rnn_output      = tf.reshape(rnn_output, [-1, self.num_out_hidden])\n",
    "            \n",
    "            self.attention  = tf.nn.softmax(tf.reshape(tf.matmul(rnn_output, self.weight_a) + self.bias_a, \n",
    "                                                       [-1, self.maximum_seq_length, 1]), axis=1)\n",
    "            \n",
    "            attn_mask       = tf.expand_dims(tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2)), axis=2)\n",
    "            masked_attn     = tf.multiply(attn_mask, self.attention)\n",
    "            attn_norms      = tf.expand_dims(tf.tile(tf.reduce_sum(masked_attn, axis=1), [1, self.maximum_seq_length]), axis=2)\n",
    "            self.attention  = masked_attn/attn_norms\n",
    "            self.attention_ = tf.tile(self.attention, [1, 1, self.input_dim])\n",
    "            self.context    = tf.reduce_sum(tf.multiply(self.attention_, self.rnn_input_), reduction_indices=1)\n",
    "            context_layer   = tf.matmul(self.context, self.weight_0) + self.bias_0\n",
    "            prediction      = tf.nn.softmax(tf.matmul(context_layer, self.weight) + self.bias)\n",
    "        \n",
    "        elif self.model_type == 'Seq2SeqAttention':\n",
    "            \n",
    "            self.attention  = self.seq2seq_attn\n",
    "            \n",
    "            attn_mask       = tf.expand_dims(tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2)), axis=2)\n",
    "            masked_attn     = tf.multiply(attn_mask, self.attention)\n",
    "            attn_norms      = tf.expand_dims(tf.tile(tf.reduce_sum(masked_attn, axis=1), [1, self.maximum_seq_length]), axis=2)\n",
    "            self.attention  = masked_attn/attn_norms\n",
    "            self.attention_ = tf.tile(self.attention, [1, 1, self.input_dim])\n",
    "            self.context    = tf.reduce_sum(tf.multiply(self.attention_, self.rnn_input_), reduction_indices=1)\n",
    "            context_layer   = tf.matmul(self.context, self.weight_0) + self.bias_0\n",
    "            prediction      = tf.nn.softmax(tf.matmul(context_layer, self.weight) + self.bias)\n",
    "\n",
    "        prediction      = tf.reshape(prediction, [-1, self.maximum_seq_length, self.output_dim])\n",
    "        self.predicted  = prediction\n",
    "        self.predicted  = tf.identity(self.predicted, name=self.output_name)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    \n",
    "    def process_rnn_inputs(self):\n",
    "        \n",
    "        if self.model_type in ['RETAIN', 'Seq2SeqAttention']: \n",
    "            \n",
    "            self.num_samples = tf.shape(self.data)[0]\n",
    "             \n",
    "            Lengths_         = np.repeat(self.length, self.maximum_seq_length)\n",
    "            \n",
    "            conv_data        = tf.reshape(tf.tile(self.data, [1, self.maximum_seq_length, 1]), \n",
    "                                          [self.maximum_seq_length * self.num_samples, \n",
    "                                           self.maximum_seq_length, self.input_dim])\n",
    "            \n",
    "            conv_mask_       = tf.ones([self.maximum_seq_length, self.maximum_seq_length], tf.float32)\n",
    "            \n",
    "            conv_mask        = tf.tile(tf.expand_dims(tf.tile(tf.matrix_band_part(conv_mask_, -1, 0), \n",
    "                                                              [self.num_samples, 1]), 2), \n",
    "                                                              [1, 1, self.input_dim])\n",
    "            \n",
    "            masked_data   = tf.multiply(conv_data, conv_mask)\n",
    "            \n",
    "            Seq_lengths_  = tf.tile(tf.range(1, self.maximum_seq_length + 1, 1), [self.num_samples])\n",
    "            \n",
    "            if self.model_type == 'RETAIN':\n",
    "            \n",
    "                self.rnn_input_  = tf.reverse_sequence(masked_data, batch_axis=0, seq_dim=1, \n",
    "                                                       seq_lengths=Seq_lengths_, seq_axis=None)\n",
    "            else:\n",
    "                \n",
    "                self.rnn_input_  = masked_data\n",
    "                \n",
    "            \n",
    "            used         = tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2))\n",
    "            length       = tf.reduce_sum(used, reduction_indices=1)\n",
    "            self.length_ = tf.cast(length, tf.int32)\n",
    "            \n",
    "            self.target_ = tf.tile(self.target, [self.maximum_seq_length, 1, 1]) \n",
    "            \n",
    "        else:    \n",
    "            \n",
    "            self.rnn_input_ = self.rnn_input\n",
    "            self.target_    = self.target \n",
    "            self.length_    = self.length\n",
    "\n",
    "    @lazy_property\n",
    "    def loss(self):\n",
    "        \n",
    "        # Compute cross entropy for each frame.\n",
    "        cross_entropy  = tf.reduce_sum(-1*(self.target * tf.log(self.prediction) + (1-self.target)*(tf.log(1-self.prediction))),\n",
    "                                       reduction_indices=2) \n",
    "\n",
    "        mask           = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "        self.mask      = mask\n",
    "        \n",
    "        # Average over actual sequence lengths.\n",
    "        cross_entropy  = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32)\n",
    "        \n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, X, Y, T=None):\n",
    "        \n",
    "        X_, Y_   = padd_data(X, self.maximum_seq_length), padd_data(Y, self.maximum_seq_length)\n",
    "        \n",
    "        if T is not None:\n",
    "            T_   = padd_data(T, self.maximum_seq_length)\n",
    "        \n",
    "        \n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        opt      = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        init     = tf.global_variables_initializer()\n",
    "        \n",
    "        sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "                \n",
    "            for _ in range(self.num_iterations):\n",
    "                \n",
    "                batch_samples = np.random.choice(list(range(X_.shape[0])), size=self.batch_size, replace=False)\n",
    "                batch_train   = X_[batch_samples,:,:]\n",
    "                batch_targets = Y_[batch_samples,:,:] \n",
    "                \n",
    "                if T is not None:\n",
    "                    batch_times = T_[batch_samples,:,:]\n",
    "                    \n",
    "                    train_dict  = {self.data   : batch_train,\n",
    "                                   self.target : batch_targets,\n",
    "                                   self.times  : batch_times}\n",
    "                else:\n",
    "                    \n",
    "                    train_dict  = {self.data   : batch_train,\n",
    "                                   self.target : batch_targets}\n",
    "                \n",
    "                \n",
    "                sess.run(opt, feed_dict=train_dict)\n",
    "                \n",
    "                Loss          = sess.run(self.loss, feed_dict=train_dict)\n",
    "                \n",
    "                self._Losses.append(Loss)\n",
    " \n",
    "                # Visualize function\n",
    "                print('Epoch {} \\t----- \\tBatch {} \\t----- \\tLoss {}'.format(epoch, _, self._Losses[-1]))\n",
    "  \n",
    "        # change names\n",
    "        saver.save(sess, \"./mlaimRNN_model\") \n",
    "        \n",
    "        if os.path.exists(\"modelgraph\"):\n",
    "            \n",
    "            shutil.rmtree(\"modelgraph\")\n",
    "        \n",
    "        tf.saved_model.simple_save(sess, export_dir='modelgraph', inputs={\"myInput\": self.data}, \n",
    "                                   outputs={\"myOutput\": self.predicted})    \n",
    "        \n",
    "            \n",
    "    def predict(self, X, T=None):\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            saver           = tf.train.import_meta_graph(\"mlaimRNN_model.meta\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "            \n",
    "            preds_lengths   = [len(X[k]) for k in range(len(X))]\n",
    "            \n",
    "            X_pred          = padd_data(X, padd_length=self.maximum_seq_length)\n",
    "            \n",
    "            if T is not None:\n",
    "                T_pred      = padd_data_enforce(T, padd_length=self.maximum_seq_length)\n",
    "                pred_dict   = {self.data   : X_pred, self.times   : T_pred}\n",
    "            else:\n",
    "                pred_dict   = {self.data   : X_pred}\n",
    "            \n",
    "            prediction_     = sess.run(self.prediction, pred_dict).reshape([-1, self.maximum_seq_length, 1])         \n",
    "\n",
    "            preds_          = []\n",
    "            \n",
    "            for k in range(len(X)):\n",
    "                \n",
    "                preds_.append(prediction_[k, 0 : preds_lengths[k]])\n",
    "                \n",
    "                \n",
    "            if self.model_type in ['RETAIN', 'Seq2SeqAttention']: \n",
    "                \n",
    "                attn_                  = sess.run(self.attention, pred_dict) \n",
    "                attn_per_patient       = [attn_[u * self.maximum_seq_length : u * self.maximum_seq_length + self.maximum_seq_length, :, :] for u in range(len(X))]\n",
    "                attn_lists_per_patient = [[attn_per_patient[u][k, 0 : k + 1, :] for k in range(self.maximum_seq_length)] for u in range(len(X))]\n",
    "                \n",
    "                preds_                 = (preds_, attn_lists_per_patient)\n",
    "            \n",
    "        return preds_    \n",
    "    \n",
    "    \n",
    "    def evaluate(self, preds, Y_test):\n",
    "        \n",
    "        flat_preds   = flatten_sequences_to_numpy(preds)\n",
    "        flat_Y_test  = np.array(list(itertools.chain.from_iterable([Y_test[k].tolist() for k in range(len(Y_test))])))\n",
    "        \n",
    "        _performance = roc_auc_score(flat_Y_test, flat_preds)\n",
    "        \n",
    "        return _performance\n",
    "    \n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size, wnames):\n",
    "    \n",
    "        weight = tf.get_variable(wnames[0], shape=[in_size, out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias   = tf.get_variable(wnames[1], shape=[out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        return weight, bias\n",
    "        \n",
    "\n",
    "\n",
    "class sequence_prediction:\n",
    "    \n",
    "    '''\n",
    "    Parent class for all RNN models.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 maximum_seq_length, \n",
    "                 input_dim, \n",
    "                 model_type='RNN',\n",
    "                 rnn_type='RNN',\n",
    "                 input_name=\"Input\", \n",
    "                 output_name=\"Output\",\n",
    "                 model_name=\"SeqModel\",\n",
    "                 num_iterations=20, \n",
    "                 num_epochs=10, \n",
    "                 batch_size=100, \n",
    "                 learning_rate=0.0005, \n",
    "                 num_rnn_hidden=200, \n",
    "                 num_rnn_layers=1,\n",
    "                 dropout_keep_prob=None,\n",
    "                 num_out_hidden=200, \n",
    "                 num_out_layers=1,\n",
    "                 verbosity=True,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        \n",
    "        # Set all model variables\n",
    "\n",
    "        self.maximum_seq_length = maximum_seq_length \n",
    "        self.input_dim          = input_dim\n",
    "        self.output_dim         = input_dim\n",
    "        self.model_type         = model_type\n",
    "        self.rnn_type           = rnn_type\n",
    "        \n",
    "        self.input_name         = input_name \n",
    "        self.output_name        = output_name \n",
    "        self.model_name         = model_name\n",
    "        self.num_iterations     = num_iterations\n",
    "        self.num_epochs         = num_epochs\n",
    "        self.batch_size         = batch_size\n",
    "        self.learning_rate      = learning_rate\n",
    "        self.num_rnn_hidden     = num_rnn_hidden\n",
    "        self.num_rnn_layers     = num_rnn_layers\n",
    "        self.dropout_keep_prob  = dropout_keep_prob\n",
    "        self.num_out_hidden     = num_out_hidden\n",
    "        self.num_out_layers     = num_out_layers\n",
    "        self.verbosity          = verbosity\n",
    "        \n",
    "        \n",
    "        self.build_rnn_model()\n",
    "        tf.reset_default_graph()\n",
    "        self.build_rnn_graph()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def build_rnn_model(self):\n",
    "        \n",
    "        # replace this with dictionary style indexing\n",
    "        \n",
    "        model_options_names     = ['RNN','LSTM','GRU','PhasedLSTM']\n",
    "        \n",
    "        optimizer_options_names = []\n",
    "        \n",
    "        \n",
    "        model_options   = [BasicRNNCell(self.num_rnn_hidden), rnn_cell.LSTMCell(self.num_rnn_hidden), \n",
    "                                   rnn_cell.GRUCell(self.num_rnn_hidden), PhasedLSTMCell(self.num_rnn_hidden)]\n",
    "        \n",
    "        self._rnn_model = model_options[np.where(np.array(model_options_names)==self.rnn_type)[0][0]]\n",
    "        \n",
    "        if self.dropout_keep_prob is not None:\n",
    "            \n",
    "            self._rnn_model = tf.nn.rnn_cell.DropoutWrapper(self._rnn_model, output_keep_prob=self.dropout_keep_prob)\n",
    "        \n",
    "        self._Losses = []\n",
    "        \n",
    "\n",
    "    def build_rnn_graph(self):\n",
    "        \n",
    "        self.data   = tf.placeholder(tf.float32, \n",
    "                                     [None, self.maximum_seq_length, self.input_dim], \n",
    "                                     name=self.input_name)\n",
    "            \n",
    "        self.target = tf.placeholder(tf.float32, \n",
    "                                     [None, self.maximum_seq_length, self.output_dim]) \n",
    "        \n",
    "            \n",
    "        self.rnn_input  = self.data \n",
    "\n",
    "            \n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        \n",
    "        used   = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        \n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        \n",
    "        self.process_rnn_inputs()\n",
    "        \n",
    "        # Recurrent network.   \n",
    "        if self.model_type != 'Seq2SeqAttention': \n",
    "            \n",
    "            rnn_output, _  = rnn.dynamic_rnn(self._rnn_model, \n",
    "                                             self.rnn_input_, \n",
    "                                             dtype=tf.float32, \n",
    "                                             sequence_length=self.length_,)\n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "                tf.nn.rnn_cell = tf.contrib.rnn\n",
    "                tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "                print(\"TensorFlow version : >= 1.0\")\n",
    "            \n",
    "            except: \n",
    "            \n",
    "                print(\"TensorFlow version : 0.12\")\n",
    "            \n",
    "            self.enc_inp    = [self.rnn_input_[:, t, :] for t in range(self.maximum_seq_length)]\n",
    "\n",
    "            self.dec_output = [tf.placeholder(tf.float32, shape=(None, 1), \n",
    "                                              name=\"dec_output_\".format(t)) for t in range(self.maximum_seq_length)]\n",
    "\n",
    "            self.dec_inp    = [tf.zeros_like(self.enc_inp[0], dtype=np.float32, name=\"GO\")] + self.enc_inp[:-1] \n",
    "\n",
    "            self.cells = []\n",
    "    \n",
    "            for i in range(self.num_rnn_layers):\n",
    "                \n",
    "                with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                    \n",
    "                    self.cells.append(tf.nn.rnn_cell.GRUCell(self.num_rnn_hidden))\n",
    "            \n",
    "            \n",
    "            self.cell  = tf.nn.rnn_cell.MultiRNNCell(self.cells)\n",
    "            self.dec_outputs, self.dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(self.enc_inp, self.dec_inp, self.cell) \n",
    "            \n",
    "            self.weight_dec, self.bias_dec = self._weight_and_bias(self.num_rnn_hidden, self.output_dim, [\"w_dec\", \"b_dec\"])\n",
    "            \n",
    "            self.seq2seq_attn = [(tf.matmul(i, self.weight_dec) + self.bias_dec) for i in self.dec_outputs]\n",
    "            self.seq2seq_attn = tf.nn.softmax(tf.reshape(tf.stack(self.seq2seq_attn), \n",
    "                                                         [-1, self.maximum_seq_length, 1]), axis=1)\n",
    "            \n",
    "        \n",
    "        # Softmax layer.\n",
    "        self.weight_0, self.bias_0 = self._weight_and_bias(self.input_dim, \n",
    "                                                           self.num_out_hidden, \n",
    "                                                           [\"w_0\", \"b_0\"])\n",
    "        \n",
    "        self.weight, self.bias     = self._weight_and_bias(self.num_out_hidden, \n",
    "                                                           self.output_dim, \n",
    "                                                           [\"w\", \"b\"])\n",
    "            \n",
    "        # Flatten to apply same weights to all time steps.\n",
    "        \n",
    "        if self.model_type not in ['RETAIN', 'Seq2SeqAttention']: \n",
    "            \n",
    "            rnn_output  = tf.reshape(rnn_output, [-1, self.num_out_hidden])\n",
    "            \n",
    "            prediction  = tf.matmul(rnn_output, self.weight) + self.bias\n",
    "        \n",
    "        elif self.model_type == 'RETAIN':\n",
    "            \n",
    "            self.weight_a, self.bias_a = self._weight_and_bias(self.num_out_hidden, 1, [\"w_a\", \"b_a\"])\n",
    "            \n",
    "            rnn_output      = tf.reshape(rnn_output, [-1, self.num_out_hidden])\n",
    "            \n",
    "            self.attention  = tf.nn.softmax(tf.reshape(tf.matmul(rnn_output, self.weight_a) + self.bias_a, \n",
    "                                                       [-1, self.maximum_seq_length, 1]), axis=1)\n",
    "            \n",
    "            attn_mask       = tf.expand_dims(tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2)), axis=2)\n",
    "            masked_attn     = tf.multiply(attn_mask, self.attention)\n",
    "            attn_norms      = tf.expand_dims(tf.tile(tf.reduce_sum(masked_attn, axis=1), [1, self.maximum_seq_length]), axis=2)\n",
    "            self.attention  = masked_attn #/attn_norms\n",
    "            self.attention_ = tf.tile(self.attention, [1, 1, self.input_dim])\n",
    "            self.context    = tf.reduce_sum(tf.multiply(self.attention_, self.rnn_input_), reduction_indices=1)\n",
    "            context_layer   = tf.matmul(self.context, self.weight_0) + self.bias_0\n",
    "            prediction      = tf.matmul(context_layer, self.weight) + self.bias\n",
    "        \n",
    "        elif self.model_type == 'Seq2SeqAttention':\n",
    "            \n",
    "            self.attention  = self.seq2seq_attn\n",
    "            \n",
    "            attn_mask       = tf.expand_dims(tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2)), axis=2)\n",
    "            masked_attn     = tf.multiply(attn_mask, self.attention)\n",
    "            attn_norms      = tf.expand_dims(tf.tile(tf.reduce_sum(masked_attn, axis=1), [1, self.maximum_seq_length]), axis=2)\n",
    "            self.attention  = masked_attn/attn_norms\n",
    "            self.attention_ = tf.tile(self.attention, [1, 1, self.input_dim])\n",
    "            self.context    = tf.reduce_sum(tf.multiply(self.attention_, self.rnn_input_), reduction_indices=1)\n",
    "            context_layer   = tf.matmul(self.context, self.weight_0) + self.bias_0\n",
    "            prediction      = tf.matmul(context_layer, self.weight) + self.bias\n",
    "\n",
    "        prediction      = tf.reshape(prediction, [-1, self.maximum_seq_length, self.output_dim])\n",
    "        self.predicted  = prediction\n",
    "        self.predicted  = tf.identity(self.predicted, name=self.output_name)\n",
    "        \n",
    "\n",
    "        return prediction\n",
    "\n",
    "    \n",
    "    def process_rnn_inputs(self):\n",
    "        \n",
    "        if self.model_type in ['RETAIN', 'Seq2SeqAttention']: \n",
    "            \n",
    "            self.num_samples = tf.shape(self.data)[0]\n",
    "             \n",
    "            Lengths_         = np.repeat(self.length, self.maximum_seq_length)\n",
    "            \n",
    "            conv_data        = tf.reshape(tf.tile(self.data, [1, self.maximum_seq_length, 1]), \n",
    "                                          [self.maximum_seq_length * self.num_samples, \n",
    "                                           self.maximum_seq_length, self.input_dim])\n",
    "            \n",
    "            conv_mask_       = tf.ones([self.maximum_seq_length, self.maximum_seq_length], tf.float32)\n",
    "            \n",
    "            conv_mask        = tf.tile(tf.expand_dims(tf.tile(tf.matrix_band_part(conv_mask_, -1, 0), \n",
    "                                                              [self.num_samples, 1]), 2), \n",
    "                                                              [1, 1, self.input_dim])\n",
    "            \n",
    "            masked_data   = tf.multiply(conv_data, conv_mask)\n",
    "            \n",
    "            Seq_lengths_  = tf.tile(tf.range(1, self.maximum_seq_length + 1, 1), [self.num_samples])\n",
    "            \n",
    "            if self.model_type == 'RETAIN':\n",
    "            \n",
    "                self.rnn_input_  = tf.reverse_sequence(masked_data, batch_axis=0, seq_dim=1, \n",
    "                                                       seq_lengths=Seq_lengths_, seq_axis=None)\n",
    "            else:\n",
    "                \n",
    "                self.rnn_input_  = masked_data\n",
    "                \n",
    "            \n",
    "            used         = tf.sign(tf.reduce_max(tf.abs(self.rnn_input_), reduction_indices=2))\n",
    "            length       = tf.reduce_sum(used, reduction_indices=1)\n",
    "            self.length_ = tf.cast(length, tf.int32)\n",
    "            \n",
    "            self.target_ = tf.tile(self.target, [self.maximum_seq_length, 1, 1]) \n",
    "            \n",
    "        else:    \n",
    "            \n",
    "            self.rnn_input_ = self.rnn_input\n",
    "            self.target_    = self.target \n",
    "            self.length_    = self.length\n",
    "\n",
    "    @lazy_property\n",
    "    def loss(self):\n",
    "\n",
    "        MSE            = (self.target - self.prediction)**2\n",
    "        \n",
    "        return tf.reduce_mean(MSE) \n",
    "        \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        self.normalizer   = MinMaxScaler()\n",
    "        self.normalizer.fit(np.concatenate(X))\n",
    "\n",
    "        self.X_normalized  = []\n",
    "        self.Y_normalized  = []\n",
    "\n",
    "        for k in range(len(X)):\n",
    "            \n",
    "            self.X_normalized.append(self.normalizer.transform(X[k])[:len(X[k])-1,:]) \n",
    "            self.Y_normalized.append(self.normalizer.transform(X[k])[1:,:]) \n",
    "        \n",
    "        \n",
    "        X_  = padd_data(self.X_normalized, self.maximum_seq_length)\n",
    "        Y_  = padd_data(self.Y_normalized, self.maximum_seq_length)\n",
    "        \n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        opt      = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        init     = tf.global_variables_initializer()\n",
    "        \n",
    "        sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "                \n",
    "            for _ in range(self.num_iterations):\n",
    "                \n",
    "                batch_samples = np.random.choice(list(range(X_.shape[0])), size=self.batch_size, replace=False)\n",
    "                batch_train   = X_[batch_samples,:,:]\n",
    "                batch_targets = Y_[batch_samples,:,:]\n",
    "                    \n",
    "                train_dict    = {self.data   : batch_train,\n",
    "                                 self.target : batch_targets}\n",
    "                \n",
    "                \n",
    "                sess.run(opt, feed_dict=train_dict)\n",
    "                \n",
    "                Loss          = sess.run(self.loss, feed_dict=train_dict)\n",
    "                \n",
    "                self._Losses.append(Loss)\n",
    " \n",
    "                # Visualize function\n",
    "                if self.verbosity:\n",
    "                    print('Epoch {} \\t----- \\tBatch {} \\t----- \\tLoss {}'.format(epoch, _, self._Losses[-1]))\n",
    "  \n",
    "        # change names\n",
    "        saver.save(sess, \"./mlaimRNN_model\") \n",
    "        \n",
    "        if os.path.exists(\"modelgraph\"):\n",
    "            \n",
    "            shutil.rmtree(\"modelgraph\")\n",
    "        \n",
    "        tf.saved_model.simple_save(sess, export_dir='modelgraph', inputs={\"myInput\": self.data}, \n",
    "                                   outputs={\"myOutput\": self.predicted})    \n",
    "        \n",
    "            \n",
    "    def predict(self, X, T=None):\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            saver           = tf.train.import_meta_graph(\"mlaimRNN_model.meta\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "            \n",
    "            preds_lengths   = [len(X[k]) for k in range(len(X))]\n",
    "\n",
    "            self.X_normalized  = []\n",
    "\n",
    "\n",
    "            for k in range(len(X)):\n",
    "            \n",
    "                self.X_normalized.append(self.normalizer.transform(X[k])[:len(X[k])-1,:]) \n",
    "\n",
    "            \n",
    "            X_pred          = padd_data(self.X_normalized, padd_length=self.maximum_seq_length)\n",
    "\n",
    "            pred_dict       = {self.data   : X_pred}\n",
    "            \n",
    "            prediction_     = sess.run(self.prediction, pred_dict).reshape([-1, self.maximum_seq_length, self.output_dim])         \n",
    "\n",
    "            preds_          = []\n",
    "            \n",
    "            for k in range(len(X)):\n",
    "                \n",
    "                preds_.append(prediction_[k, 0 : preds_lengths[k], :])\n",
    "                \n",
    "                \n",
    "            if self.model_type in ['RETAIN', 'Seq2SeqAttention']: \n",
    "                \n",
    "                attn_                  = sess.run(self.attention, pred_dict) \n",
    "                attn_per_patient       = [attn_[u * self.maximum_seq_length : u * self.maximum_seq_length + self.maximum_seq_length, :, :] for u in range(len(X))]\n",
    "                attn_lists_per_patient = [[attn_per_patient[u][k, 0 : k + 1, :] for k in range(self.maximum_seq_length)] for u in range(len(X))]\n",
    "                \n",
    "                preds_                 = (preds_, attn_lists_per_patient)\n",
    "            \n",
    "        return preds_    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size, wnames):\n",
    "    \n",
    "        weight = tf.get_variable(wnames[0], shape=[in_size, out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias   = tf.get_variable(wnames[1], shape=[out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa3b07-a161-460c-9375-1ee384d59cef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BASE DATA LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abcc54-28e8-4960-a34f-0ed145e67d93",
   "metadata": {},
   "source": [
    "### Features currently included:\n",
    "STATIC (from owner dataset):\\\n",
    "dog_id - for joining with health conditions dataset\n",
    "\n",
    "CATEGORIES:\n",
    "1. GENERAL HEALTH\n",
    "2. EXCITABILITY\n",
    "3. AGGRESSION\n",
    "4. FEAR AND ANXIETY\n",
    "5. SEPARATION-RELATED BEHAVIOR\n",
    "6. ATTACHMENT, ATTENTION SEEKING\n",
    "7. TRAINING DIFFICULTY\n",
    "8. MISCELLANEOUS BEHAVIORS\n",
    "9. DOGGO DEMOGRAPHICS-BREED\n",
    "10. OTHER DEMOGRAPHICS\n",
    "11. ENVIRONMENTAL\n",
    "12. LIFESTYLE\n",
    "13. MEDICATIONS / PREVENTIVES\n",
    "14. TEMPORAL\n",
    "\n",
    "GENERAL HEALTH\\\n",
    "[hs_general_health','Hs_recent_hospitalization']\n",
    "- hs_general_health' - 1 (Excellent) - 6 (Very poor)\n",
    "- 'hs_recent_hospitalization' - true/false\n",
    "\n",
    "EXCITABILITY\\\n",
    "['db_excitement_level_before_car_ride','db_excitement_level_before_walk']\n",
    "- 'db_excitement_level_before_car_ride',  0.0 (calm) - 4.0 (extremely excitable)\n",
    "- 'db_excitement_level_before_walk', 0.0 (calm) - 4.0 (extremely excitable)\n",
    "\n",
    "AGGRESSION\\\n",
    "['db_aggression_level_approached_while_eating','db_aggression_level_delivery_workers_at_home','db_aggression_level_food_taken_away','db_aggression_level_on_leash_unknown_dog','db_aggression_level_on_leash_unknown_human', 'db_aggression_level_toys_taken_away','db_aggression_level_unknown_aggressive_dog', 'db_aggression_level_unknown_human_near_yard']\n",
    "- 'db_aggression_level_approached_while_eating', 0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_delivery_workers_at_home', 0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_food_taken_away',  0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_on_leash_unknown_dog',  0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_on_leash_unknown_human',  0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_toys_taken_away',  0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_unknown_aggressive_dog',  0.0 (none) - 4.0 (serious)\n",
    "- 'db_aggression_level_unknown_human_near_yard',  0.0 (none) - 4.0 (serious)\n",
    "\n",
    "FEAR AND ANXIETY\\\n",
    "['db_fear_level_bathed_at_home','db_fear_level_loud_noises','db_fear_level_nails_clipped_at_home','db_fear_level_unknown_aggressive_dog','db_fear_level_unknown_dogs','db_fear_level_unknown_human_away_from_home','db_fear_level_unknown_human_touch','db_fear_level_unknown_objects_outside','db_fear_level_unknown_situations']\n",
    "- 'db_fear_level_bathed_at_home',  0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_loud_noises',  0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_nails_clipped_at_home',   0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_unknown_aggressive_dog',  0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_unknown_dogs',  0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_unknown_human_away_from_home',   0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_unknown_human_touch',   0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_unknown_objects_outside',  0.0 (none) - 4.0 (extreme fear)\n",
    "- 'db_fear_level_unknown_situations',  0.0 (none) - 4.0 (extreme fear)\n",
    "\n",
    "SEPARATION-RELATED BEHAVIOR\\\n",
    "['db_left_alone_barking_frequency','db_left_alone_restlessness_frequency','db_left_alone_scratching_frequency']\n",
    "- 'db_left_alone_barking_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_left_alone_restlessness_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_left_alone_scratching_frequency',  0.0 (never) - 4.0 (always)\n",
    "\n",
    "ATTACHMENT, ATTENTION SEEKING\\\n",
    "['db_attention_seeking_follows_humans_frequency','db_attention_seeking_sits_close_to_humans_frequency']\n",
    "- 'db_attention_seeking_follows_humans_frequency', 0.0 (never) - 4.0 (always)\n",
    "- 'db_attention_seeking_sits_close_to_humans_frequency',  0.0 (never) - 4.0 (always)\n",
    "\n",
    "TRAINING DIFFICULTY\\\n",
    "['db_training_distraction_frequency','db_training_obeys_sit_command_frequency','db_training_obeys_stay_command_frequency']\n",
    "- 'db_training_distraction_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_training_obeys_sit_command_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_training_obeys_stay_command_frequency',  0.0 (never) - 4.0 (always)\n",
    "\n",
    "MISCELLANEOUS BEHAVIORS\\\n",
    "['db_barks_frequency','db_chases_birds_frequency','db_chases_squirrels_frequency','db_chases_tail_frequency','db_chews_inappropriate_objects_frequency','db_defecates_alone_frequency','db_energetic_frequency','db_escapes_home_or_property_frequency','db_hyperactive_frequency','db_playful_frequency','db_pulls_leash_frequency','db_urinates_alone_frequency','db_urinates_in_home_frequency']\n",
    "- 'db_barks_frequency', 0.0 (never) - 4.0 (always)\n",
    "- 'db_chases_birds_frequency', 0.0 (never) - 4.0 (always)\n",
    "- 'db_chases_squirrels_frequency', 0.0 (never) - 4.0 (always)\n",
    "- 'db_chases_tail_frequency', 0.0 (never) - 4.0 (always)\n",
    "- 'db_chews_inappropriate_objects_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_defecates_alone_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_energetic_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_escapes_home_or_property_frequency',  0.0 (never) - 4.0 (always)\n",
    "- 'db_hyperactive_frequency',   0.0 (never) - 4.0 (always)\n",
    "- 'db_playful_frequency',   0.0 (never) - 4.0 (always)\n",
    "- 'db_pulls_leash_frequency',   0.0 (never) - 4.0 (always)\n",
    "- 'db_urinates_alone_frequency',   0.0 (never) - 4.0 (always)\n",
    "- 'db_urinates_in_home_frequency'  0.0 (never) - 4.0 (always)\n",
    "\n",
    "DOGGO DEMOGRAPHICS\\ \n",
    "BREED (# representing a breed)\\\n",
    "['dd_breed_pure_or_mixed', 'dd_combined_main_breed']\n",
    "- 'dd_breed_pure_or_mixed', 1 (purebred) 2 (mixed)\n",
    "- 'dd_combined_main_breed', categorical # for breed - combines these two:\n",
    "- - 'dd_breed_pure', categorical for breed, only when dog is purebred\n",
    "- - 'dd_breed_pure_non_akc', categorical for breed, only when dog is purebred and isn't in the breed_pure list\n",
    "- - 'dd_breed_mixed_primary', categorical for breed, only when dog is mixed\n",
    "\n",
    "OTHER DEMOGRAPHICS\\\n",
    "['dd_age_years','dd_spayed_or_neutered','dd_spay_or_neuter_age','dd_insurance','dd_sex','dd_weight_lbs','dd_weight_range']\n",
    "- 'dd_age_years' - 0.000 - 30.000 - take out all dogs with zero, since still growing/puppies\n",
    "- 'dd_spayed_or_neutered', 1 YES 2 NO, but look out for NaN\n",
    "- 'dd_spay_or_neuter_age', 1 (<6 mo), 2 (6-12mo), 3 (12-18mo), 4 (18-24mo), 5 (2-4yr), 6(4-6yr), 7(6-8yr), 8 (8+yr), 99(unknown)\n",
    "- 'dd_insurance' - true/false\n",
    "- 'dd_sex', 1 (M) 2(F)\n",
    "- 'dd_weight_lbs', 0.0 - 150.0 (cotinuous)\n",
    "- 'dd_weight_range' YES, need to get values for range? (categorical)\n",
    "1- 010 lbs\\\n",
    "2 - 1120 lbs\\\n",
    "3 - 2130 lbs\\\n",
    "4 - 3140 lbs\\\n",
    "5 - 4150 lbs\\\n",
    "6 - 5160 lbs\\\n",
    "7 - 6170 lbs\\\n",
    "8 - 7180 lbs\\\n",
    "9 - 8190 lbs\\\n",
    "10 - 91100 lbs\\\n",
    "11 - More than 100 lbs\n",
    "\n",
    "ENVIRONMENTAL\\\n",
    "['de_air_cleaner_present','de_air_freshener_present','de_asbestos_present','de_central_air_conditioning_present','de_central_heat_present','de_routine_toys','de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours','de_eats_feces','de_eats_grass_frequency','de_interacts_with_neighborhood_animals''de_interacts_with_neighborhood_humans','de_licks_chews_or_plays_with_non_toys']\n",
    "- 'de_air_cleaner_present', 'de_air_freshener_present', 'de_asbestos_present', 'de_central_air_conditioning_present', 'de_central_heat_present', ALL, 1 YES, 0 NO, 99 UNKNOWN\n",
    "- 'de_routine_toys' - 1 YES, 0 NO\n",
    "- 'de_daytime_sleep_avg_hours', 0-24, 99 means unknown\n",
    "- 'de_nighttime_sleep_avg_hours' 0-24, 99 means unknown\n",
    "- 'de_eats_feces', 1 YES 0 NO 99 UNKNOWN\n",
    "- 'de_eats_grass_frequency', 1 (frequently), 2 (infrequently), 0 (never)\n",
    "- 'de_interacts_with_neighborhood_animals', 1 YES 0 NO\n",
    "- 'de_interacts_with_neighborhood_humans', 1 YES 0 NO\n",
    "- 'de_licks_chews_or_plays_with_non_toys', 1 YES 0 NO\n",
    "\n",
    "LIFESTYLE\\\n",
    "['df_appetite','df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight','df_ever_underweight','df_feedings_per_day','free_fed','df_diet_consistency','df_primary_diet_component''df_treats_frequency','df_weight_change_last_year']\n",
    "- 'df_appetite',  1 (poor) 2 (avg) 3 (voracious)\n",
    "- 'df_appetite_change_last_year', 1 YES 0 NO 99 UNKNOWN\n",
    "- 'df_ever_malnourished', 1 YES 0 NO 99 UNKNOWN\n",
    "- 'df_ever_overweight', 1 YES 0 NO 99 UNKNOWN\n",
    "- 'df_ever_underweight', 1 YES 0 NO 99 UNKNOWN\n",
    "- 'df_feedings_per_day', 1 (ONCE) 2 (TWICE) 3 (3+) 4 (Free fed) -> breakout free fed as separate data point 'free_fed'\n",
    "- 'df_diet_consistency', 1 (very consistent) 2 (somewhat consistent) 3 not at all consistent) 98 OTHER\n",
    "- 'df_treats_frequency', 3 (3+/day) 2 (2X) 1 (1X) 4 (occasionally, not daily) 0 never\n",
    "- 'df_weight_change_last_year' 1 YES 0 NO 3 (dog is puppy still growing) 99 UNKNOWN\n",
    "- 'df_primary_diet_component':\\\n",
    "1 - Commercially prepared dry food (kibble)\\\n",
    "2 - Commercially prepared canned food\\\n",
    "3 - Commercially prepared freezedried food\\\n",
    "4 - Commercially prepared refrigerated or frozen raw food\\\n",
    "5 - Commercially prepared semidry or semimoist food\\\n",
    "6 - Home prepared cooked diet\\\n",
    "7 - Home prepared raw diet\\\n",
    "98 - Other\n",
    "\n",
    "MEDICATIONS / PREVENTIVES\\\n",
    "['mp_dental_examination_frequency','mp_dental_brushing_frequency','mp_dental_treat_frequency','mp_dental_food_frequency','mp_dental_breath_freshener_frequency','mp_flea_and_tick_treatment','mp_heartworm_preventative']\n",
    "- 'mp_dental_examination_frequency', 0 (never) 1 (occasionally) 2 (monthly) 3 (weekly) 4 (daily)\n",
    "- 'mp_dental_brushing_frequency', 0 (never) 1 (occasionally) 2 (monthly) 3 (weekly) 4 (daily)\n",
    "- 'mp_dental_treat_frequency', 0 (never) 1 (occasionally) 2 (monthly) 3 (weekly) 4 (daily)\n",
    "- 'mp_dental_food_frequency', 0 (never) 1 (occasionally) 2 (monthly) 3 (weekly) 4 (daily)\n",
    "- 'mp_dental_breath_freshener_frequency', 0 (never) 1 (occasi) 2 (monthly) 3 (weekly) 4 (daily)\n",
    "- 'mp_flea_and_tick_treatment', t/f\n",
    "- 'mp_heartworm_preventative', t/f\n",
    "\n",
    "TEMPORAL: (comes from health_conditions dataset)\\\n",
    "[dog_id,hs_diagnosis_month,hs_diagnosis_year,hs_follow_up_ongoing',hs_required_surgery_or_hospitalization]\\\n",
    "The main temporal data point is the condition that is diagnosis, which is basically data leakage, because it accounts for the diseases that will be predicted in the analysis.\\\n",
    "only have these available:\n",
    "- Dog_id -> to join to owner table\n",
    "- hs_diagnosis_month\t-> for reference\n",
    "- hs_diagnosis_year -> for reference\n",
    "- hs_follow_up_ongoing -> t/f\n",
    "- hs_required_surgery_or_hospitalization -> 1 (surgery) 2 (hospital) 3 (both) 4 (none)\n",
    "\n",
    "\n",
    "### Features to predict:\n",
    "['hs_health_conditions_skin','hs_health_conditions_eye','hs_health_conditions_cancer']\n",
    "\n",
    "- 'hs_health_conditions_skin', 1 YES, 2 NO \n",
    "- 'hs_health_conditions_eye',  1 YES, 2 NO\n",
    "- 'hs_health_conditions_cancer' -  1 YES, 2 NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23641d6-efb2-4c29-8d30-dba16fb28413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL DATA LOADING:\n",
    "def doggo_data_loading(seq_length=2, apply_seq_length=True, use_surg=True, data_cat=[1]):\n",
    "    \"\"\"\n",
    "    seq_length - the min # of sequences a dog can have to be included in temporal data\n",
    "    apply_seq_length - whether or not the seq_length is applied\n",
    "    use_surg - determines if the features include surgical visits in features data (part of temporal)\n",
    "    \n",
    "    data_cat parameter: takes an array of numbers representing these categories below.  \n",
    "    Each one that is included is added to the model at training as a set of features.\n",
    "    Default is to process them all!\n",
    "    \n",
    "    DATA CATEGORIES:\n",
    "    1. GENERAL HEALTH * (2)\n",
    "        [hs_general_health','hs_recent_hospitalization'] \n",
    "    2. EXCITABILITY (1)\n",
    "        ['db_excitement_level_before_walk','db_excitement_level_before_car_ride'] \n",
    "        REMOVED (1): ,\n",
    "    3. AGGRESSION (2) \n",
    "        ['db_aggression_level_approached_while_eating','db_aggression_level_on_leash_unknown_dog']\n",
    "        REMOVED (5): ,'db_aggression_level_delivery_workers_at_home','db_aggression_level_food_taken_away','db_aggression_level_on_leash_unknown_human', 'db_aggression_level_toys_taken_away','db_aggression_level_unknown_aggressive_dog', 'db_aggression_level_unknown_human_near_yard'\n",
    "    4. FEAR AND ANXIETY (2)\n",
    "        ['db_fear_level_loud_noises','db_fear_level_unknown_human_touch']\n",
    "        REMOVED (7): ,'db_fear_level_unknown_aggressive_dog','db_fear_level_bathed_at_home', 'db_fear_level_nails_clipped_at_home','db_fear_level_unknown_dogs','db_fear_level_unknown_human_away_from_home','db_fear_level_unknown_objects_outside','db_fear_level_unknown_situations'\n",
    "    5. SEPARATION-RELATED BEHAVIOR (2)\n",
    "        ['db_left_alone_barking_frequency','db_left_alone_restlessness_frequency']\n",
    "        REMOVED (1): ,'db_left_alone_scratching_frequency'\n",
    "    6. ATTACHMENT, ATTENTION SEEKING (2)\n",
    "        ['db_attention_seeking_follows_humans_frequency','db_attention_seeking_sits_close_to_humans_frequency']\n",
    "    7. TRAINING DIFFICULTY (2)\n",
    "        ['db_training_distraction_frequency','db_training_obeys_stay_command_frequency']\n",
    "        REMOVED (1): ,'db_training_obeys_sit_command_frequency'\n",
    "    8. MISCELLANEOUS BEHAVIORS (4)\n",
    "        ['db_barks_frequency','db_escapes_home_or_property_frequency','db_playful_frequency','db_urinates_in_home_frequency']\n",
    "        REMOVED (9) : ,'db_chases_birds_frequency','db_chases_squirrels_frequency','db_chases_tail_frequency','db_defecates_alone_frequency','db_urinates_alone_frequency','db_pulls_leash_frequency','db_hyperactive_frequency','db_energetic_frequency'\n",
    "    9. DOGGO DEMOGRAPHICS-BREED * (2)\n",
    "        ['dd_breed_pure_or_mixed', 'dd_combined_main_breed']\n",
    "    10. OTHER DEMOGRAPHICS * (6)\n",
    "        ['dd_age_years','dd_spayed_or_neutered','dd_spay_or_neuter_age','dd_insurance','dd_sex','dd_weight_range']\n",
    "        REMOVED (1): ,'dd_weight_lbs'\n",
    "    11. ENVIRONMENTAL (4)\n",
    "        ['de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours','de_eats_feces','de_eats_grass_frequency']\n",
    "        REMOVED (9): 'de_central_air_conditioning_present','de_central_heat_present','de_air_cleaner_present','de_air_freshener_present','de_asbestos_present','de_routine_toys','de_interacts_with_neighborhood_animals','de_interacts_with_neighborhood_humans','de_licks_chews_or_plays_with_non_toys'\n",
    "    12. LIFESTYLE * (7)\n",
    "        ['df_appetite','df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight','df_feedings_per_day','free_fed']\n",
    "        REMOVED (4): ,'df_diet_consistency','df_primary_diet_component','df_treats_frequency','df_ever_underweight','df_weight_change_last_year'\n",
    "    13. MEDICATIONS / PREVENTIVES * (3)\n",
    "        ['mp_dental_brushing_frequency','mp_flea_and_tick_treatment','mp_heartworm_preventative']\n",
    "        REMOVED (4): 'mp_dental_examination_frequency',,'mp_dental_treat_frequency','mp_dental_food_frequency','mp_dental_breath_freshener_frequency'\n",
    "\n",
    "    TEMPORAL (automatic) (2 features)\n",
    "        [dog_id,hs_diagnosis_month,hs_diagnosis_year,hs_follow_up_ongoing',hs_required_surgery_or_hospitalization] \n",
    "    \n",
    "    47 total, get rid of 1!\n",
    "    \n",
    "    PREDICTION CATEGORIES:\n",
    "    'skin' - 'hs_health_conditions_skin', 1 YES, 2 NO\n",
    "    'eye' - 'hs_health_conditions_eye', 1 YES, 2 NO\n",
    "    'cancer' - 'hs_health_conditions_cancer' - 1 YES, 2 NO\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data prep\n",
    "    # pull up data\n",
    "    heal_df = pd.read_csv(\"CSV_DATA/DAP_2020_HLES_health_condition_v1.1.csv\")\n",
    "    ownr_df = pd.read_csv(\"CSV_DATA/DAP_2020_HLES_dog_owner_v1.1.csv\")\n",
    "    \n",
    "    # pull together data based on data_cat array:\n",
    "    # also massage any data that needs massaging in each of these categories (only do if needed)\n",
    "    data_list = []\n",
    "    \n",
    "    # add prediction columns:\n",
    "    data_list.extend(('dog_id','hs_health_conditions_skin','hs_health_conditions_eye','hs_health_conditions_cancer'))\n",
    "    \n",
    "    #3\n",
    "    \n",
    "    temporal = []\n",
    "    \n",
    "    # ALT APPROACH:\n",
    "    # data_list.extend(('hs_general_health',\n",
    "    #                   'db_aggression_level_approached_while_eating',\n",
    "    #                   'db_left_alone_restlessness_frequency',\n",
    "    #                   'db_attention_seeking_follows_humans_frequency',\n",
    "    #                   'db_training_obeys_stay_command_frequency',\n",
    "    #                   'db_barks_frequency',\n",
    "    #                   'db_escapes_home_or_property_frequency',\n",
    "    #                   'db_urinates_in_home_frequency',\n",
    "    #                   'dd_breed_pure_or_mixed',\n",
    "    #                   'dd_breed_pure','dd_breed_pure_non_akc','dd_breed_mixed_primary',\n",
    "    #                   'dd_age_years',\n",
    "    #                   'dd_spayed_or_neutered',\n",
    "    #                   'dd_spay_or_neuter_age',\n",
    "    #                   'dd_insurance',\n",
    "    #                   'dd_sex',\n",
    "    #                   'dd_weight_range',\n",
    "    #                   'de_eats_feces',\n",
    "    #                   'df_appetite',\n",
    "    #                   'mp_dental_brushing_frequency',\n",
    "    #                   'mp_flea_and_tick_treatment',\n",
    "    #                   'mp_heartworm_preventative'\n",
    "    #                  ))\n",
    "    # ALL\n",
    "    data_list.extend(('dd_breed_pure_or_mixed',\n",
    "                      'dd_breed_pure','dd_breed_pure_non_akc','dd_breed_mixed_primary','df_ever_underweight',\n",
    "                    'hs_recent_hospitalization','dd_spayed_or_neutered','dd_spay_or_neuter_age',\n",
    "                      'dd_insurance','de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours','de_eats_feces',\n",
    "                      'df_appetite','df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight','df_feedings_per_day','free_fed',\n",
    "                      'mp_flea_and_tick_treatment','mp_heartworm_preventative',\n",
    "                      'df_primary_diet_component',\n",
    "                      'mp_dental_brushing_frequency',\n",
    "                      'db_excitement_level_before_walk','db_excitement_level_before_car_ride',\n",
    "                      'db_aggression_level_approached_while_eating','db_aggression_level_on_leash_unknown_dog',\n",
    "                      'db_left_alone_barking_frequency','db_left_alone_restlessness_frequency',\n",
    "                      'db_fear_level_loud_noises','db_fear_level_unknown_human_touch',\n",
    "                      'db_attention_seeking_follows_humans_frequency','db_attention_seeking_sits_close_to_humans_frequency',\n",
    "                      'hs_general_health','dd_age_years','dd_sex','dd_weight_range',\n",
    "                      'de_eats_grass_frequency',\n",
    "                      'db_training_distraction_frequency','db_training_obeys_stay_command_frequency',\n",
    "                      'db_barks_frequency','db_escapes_home_or_property_frequency','db_playful_frequency','db_urinates_in_home_frequency',\n",
    "                     ))\n",
    "    \n",
    "    # REMOVE: 'hs_recent_hospitalization'\n",
    "    # REMOVE: 'db_excitement_level_before_walk','db_excitement_level_before_car_ride'\n",
    "    # REMOVE: 'db_aggression_level_on_leash_unknown_dog'\n",
    "    # REMOVE: 'db_left_alone_barking_frequency',\n",
    "    # REMOVE: 'db_fear_level_loud_noises','db_fear_level_unknown_human_touch'\n",
    "    # REMOVE: 'db_attention_seeking_sits_close_to_humans_frequency'\n",
    "    # REMOVE: 'db_training_distraction_frequency'\n",
    "    # REMOVE: 'db_playful_frequency'\n",
    "    # REMOVE: 'de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours','de_eats_grass_frequency'\n",
    "    # REMOVE: 'df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight','df_feedings_per_day','free_fed'\n",
    "    \n",
    "#     if 1 in data_cat:\n",
    "#         print('using GENERAL HEALTH') #2\n",
    "#         data_list.extend(('hs_general_health','hs_recent_hospitalization'))\n",
    "#     if 2 in data_cat:\n",
    "#         print('using EXCITABILITY') #2\n",
    "#         data_list.extend(('db_excitement_level_before_walk','db_excitement_level_before_car_ride'))\n",
    "#     if 3 in data_cat:\n",
    "#         print('using AGGRESSION') #2\n",
    "#         data_list.extend(('db_aggression_level_approached_while_eating','db_aggression_level_on_leash_unknown_dog'))\n",
    "#     if 4 in data_cat:\n",
    "#         print('using FEAR AND ANXIETY') #2\n",
    "#         data_list.extend(('db_fear_level_loud_noises','db_fear_level_unknown_human_touch'))\n",
    "#     if 5 in data_cat:\n",
    "#         print('using SEPARATION-RELATED BEHAVIOR') #2\n",
    "#         data_list.extend(('db_left_alone_barking_frequency','db_left_alone_restlessness_frequency'))\n",
    "#     if 6 in data_cat:\n",
    "#         print('using ATTACHMENT, ATTENTION SEEKING') #2\n",
    "#         data_list.extend(('db_attention_seeking_follows_humans_frequency','db_attention_seeking_sits_close_to_humans_frequency'))\n",
    "#     if 7 in data_cat:\n",
    "#         print('using TRAINING DIFFICULTY') #2\n",
    "#         data_list.extend(('db_training_distraction_frequency','db_training_obeys_stay_command_frequency'))\n",
    "#     if 8 in data_cat:\n",
    "#         print('using MISCELLANEOUS BEHAVIORS') #4\n",
    "#         data_list.extend(('db_barks_frequency','db_escapes_home_or_property_frequency','db_playful_frequency','db_urinates_in_home_frequency'))\n",
    "#     if 9 in data_cat:\n",
    "#         print('using DEMOGRAPHICS-BREED') #2 (consolidated from 4)\n",
    "#         data_list.extend(('dd_breed_pure_or_mixed','dd_breed_pure','dd_breed_pure_non_akc','dd_breed_mixed_primary'))        \n",
    "#     if 10 in data_cat:\n",
    "#         print('using OTHER DEMOGRAPHICS') #6\n",
    "#         data_list.extend(('dd_age_years','dd_spayed_or_neutered','dd_spay_or_neuter_age','dd_insurance','dd_sex','dd_weight_range'))\n",
    "#     if 11 in data_cat:\n",
    "#         print('using ENVIRONMENTAL')  #4\n",
    "#         data_list.extend(('de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours','de_eats_feces','de_eats_grass_frequency'))\n",
    "#     if 12 in data_cat:\n",
    "#         print('using LIFESTYLE') #6\n",
    "#         data_list.extend(('df_appetite','df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight','df_feedings_per_day','free_fed'))\n",
    "#     if 13 in data_cat: \n",
    "#         print('using MEDICATIONS / PREVENTIVES') #3\n",
    "#         data_list.extend(('mp_dental_brushing_frequency','mp_flea_and_tick_treatment','mp_heartworm_preventative'))\n",
    "    \n",
    "    # make temporal automatic\n",
    "    print('using TEMPORAL')\n",
    "    temporal.extend(('dog_id','hs_condition_type','hs_follow_up_ongoing','hs_required_surgery_or_hospitalization')) # 'hs_diagnosis_month','hs_diagnosis_year',\n",
    "    \n",
    "    feat_num_static = len(data_list)\n",
    "    # these are not features (so reduce by 2):\n",
    "    # 'dog_id', 'hs_health_conditions_skin','hs_health_conditions_eye','hs_health_conditions_cancer'\n",
    "    # will use the two health conditions that are not being predicted as features!\n",
    "    feat_num_static = feat_num_static - 2\n",
    "    feat_list = data_list\n",
    "    # print('feat_num_static: ', feat_num_static)\n",
    "#     print('static featurse: ', data_list)\n",
    "    \n",
    "    feat_num_temporal = len(temporal)\n",
    "    # these are not features (so reduce by 1): 'dog_id'\n",
    "    feat_num_temporal = feat_num_temporal - 1\n",
    "    feat_list.extend(temporal)\n",
    "    # print('feat_num_temporal: ', feat_num_temporal)\n",
    "#     print('temporal features: ', temporal)\n",
    "    feat_num = feat_num_static + feat_num_temporal\n",
    "    # print('feat_num total initial: ', feat_num)\n",
    "#     print('feat_list:', feat_list)\n",
    "    \n",
    "    selected_df = ownr_df[ownr_df.columns.intersection(data_list)]\n",
    "    temporal_df = heal_df[heal_df.columns.intersection(temporal)]\n",
    "    \n",
    "    # join dataframes using doggo id\n",
    "    joined_df = selected_df.set_index('dog_id').join(temporal_df.set_index('dog_id'))\n",
    "    # NOTE! Might want to take out these: 'hs_follow_up_ongoing','hs_required_surgery_or_hospitalization'\n",
    "    #    BC those are usually a result of the disease and so a form of data leakage\n",
    "    \n",
    "    # set up temporal based disease stage\n",
    "    joined_df['skin_dev'] = (joined_df['hs_condition_type']==4).astype(int)\n",
    "    joined_df['eye_dev'] = (joined_df['hs_condition_type']==1).astype(int)\n",
    "    \n",
    "    # pull from cancer db and cross reference with dog id\n",
    "    # Can get month and year from: 'hs_follow_up_ongoing', 'hs_initial_diagnosis_month', 'hs_initial_diagnosis_year'\n",
    "    # tie those to the month and year in the health sequence data.\n",
    "    \n",
    "    # joined_df['cancer_dev'] = (joined_df['hs_condition_type']==4).astype(int)\n",
    "    \n",
    "    # print('dataframe shape 1: ', joined_df.shape)# [0]\n",
    "    # print('feature data points: ', feat_num)\n",
    "    \n",
    "    # Update dog breeds if that category selected (have to before dropping NaN)\n",
    "    if 9 in data_cat:\n",
    "        # print('removing breed cols')\n",
    "        joined_df['dd_breed_pure'] = joined_df['dd_breed_pure'].fillna(0)\n",
    "        joined_df['dd_breed_pure_non_akc'] = joined_df['dd_breed_pure_non_akc'].fillna(0)\n",
    "        joined_df['dd_breed_mixed_primary'] = joined_df['dd_breed_mixed_primary'].fillna(0)\n",
    "        \n",
    "        # add together for new\n",
    "        temp_df = joined_df[['dd_breed_pure','dd_breed_pure_non_akc','dd_breed_mixed_primary']]\n",
    "        sum_df = temp_df.sum(axis=1)\n",
    "        joined_df['dd_combined_main_breed'] = sum_df\n",
    "        feat_list.append('dd_combined_main_breed')\n",
    "        \n",
    "        # remove old columns\n",
    "        drop_cols = ['dd_breed_pure','dd_breed_pure_non_akc','dd_breed_mixed_primary']\n",
    "        joined_df = joined_df.drop(columns=drop_cols) # , axis=1\n",
    "        \n",
    "        # remove these, update feat_num (only by 2 though, since adding a new one)\n",
    "        feat_num = feat_num - 2\n",
    "        feat_list.remove('dd_breed_pure')\n",
    "        feat_list.remove('dd_breed_pure_non_akc')\n",
    "        feat_list.remove('dd_breed_mixed_primary')\n",
    "    \n",
    "    # clean out NaN\n",
    "    # for temporal data, if all NaN, then just means wasn't in health records, will take out for now,\n",
    "    #    but may need to come back and include as more samples of healthy dogs\n",
    "    joined_df = joined_df.dropna()\n",
    "    \n",
    "    # reduce to only dogs with seq_lenth+ of records # turning off for now\n",
    "#     joined_df['dog_id'] = joined_df.index  \n",
    "    joined_df = joined_df.reset_index(level=0)\n",
    "    if apply_seq_length==True:\n",
    "        joined_df = joined_df[joined_df.groupby('dog_id')['dog_id'].transform('size') >= seq_length]\n",
    "    else: \n",
    "        joined_df = joined_df\n",
    "        \n",
    "     # massage data into format better for analysis\n",
    "    if 1 in data_cat:\n",
    "        # GENERAL HEALTH'\n",
    "        # 'hs_general_health','hs_recent_hospitalization'\n",
    "        # convert t/f to 1/0\n",
    "        joined_df['hs_recent_hospitalization'] = joined_df['hs_recent_hospitalization'].astype(int)    \n",
    "    \n",
    "    if 10 in data_cat:\n",
    "        # OTHER DEMOGRAPHICS\n",
    "        # 'dd_age_years','dd_spayed_or_neutered','dd_spay_or_neuter_age','dd_insurance',\n",
    "        # 'dd_sex','dd_weight_lbs','dd_weight_range'\n",
    "        joined_df['dd_insurance'] = joined_df['hs_follow_up_ongoing'].astype(int)\n",
    "        joined_df['dd_spayed_or_neutered'] = joined_df['dd_spayed_or_neutered'].astype(int)\n",
    "        # TODO - add these back in some capacity!\n",
    "        # for now, remove unknown neutered age: dd_spay_or_neuter_age\n",
    "        joined_df.drop(joined_df[joined_df.dd_spay_or_neuter_age == 99.0].index, inplace=True)\n",
    "        \n",
    "    if 11 in data_cat:\n",
    "        # ENVIRONMENTAL \n",
    "        # ['de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours','de_eats_feces','de_eats_grass_frequency']\n",
    "        # 'de_air_cleaner_present','de_air_freshener_present','de_asbestos_present',\n",
    "        # 'de_central_air_conditioning_present','de_central_heat_present','de_routine_toys',\n",
    "        # 'de_daytime_sleep_avg_hours','de_nighttime_sleep_avg_hours',\n",
    "        # 'de_eats_feces','de_eats_grass_frequency','de_interacts_with_neighborhood_animals',\n",
    "        # 'de_interacts_with_neighborhood_humans','de_licks_chews_or_plays_with_non_toys'\n",
    "        # joined_df['de_air_cleaner_present'] = np.where(joined_df['de_air_cleaner_present'] == 99, 0, 1)\n",
    "        # joined_df['de_air_freshener_present'] = np.where(joined_df['de_air_freshener_present'] == 99, 0, 1)\n",
    "        # joined_df['de_asbestos_present'] = np.where(joined_df['de_asbestos_present'] == 99, 0, 1)\n",
    "        # joined_df['de_central_air_conditioning_present'] = np.where(joined_df['de_central_air_conditioning_present'] == 99, 0, 1)\n",
    "        # joined_df['de_central_heat_present'] = np.where(joined_df['de_central_heat_present'] == 99, 0, 1)        \n",
    "        # joined_df['de_routine_toys'] = joined_df['de_routine_toys'].astype(int)\n",
    "        joined_df['de_daytime_sleep_avg_hours'] = np.where(joined_df['de_daytime_sleep_avg_hours'] == 99, 0, 1)  \n",
    "        joined_df['de_nighttime_sleep_avg_hours'] = np.where(joined_df['de_nighttime_sleep_avg_hours'] == 99, 0, 1)  \n",
    "        joined_df['de_eats_feces'] = np.where(joined_df['de_eats_feces'] == 99, 0, 1)  \n",
    "#         'de_eats_grass_frequency', 1 (frequently), 2 (infrequently), 0 (never)        \n",
    "        # joined_df['de_interacts_with_neighborhood_animals'] = joined_df['de_interacts_with_neighborhood_animals'].astype(int)\n",
    "        # joined_df['de_interacts_with_neighborhood_humans'] = joined_df['de_interacts_with_neighborhood_humans'].astype(int)\n",
    "        # joined_df['de_licks_chews_or_plays_with_non_toys'] = joined_df['de_licks_chews_or_plays_with_non_toys'].astype(int)\n",
    "\n",
    "    if 12 in data_cat:\n",
    "        # LIFESTYLE\n",
    "        # ['df_appetite','df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight',\n",
    "        #  'df_ever_underweight','df_feedings_per_day','free_fed','df_weight_change_last_year']\n",
    "        # 'df_appetite','df_appetite_change_last_year','df_ever_malnourished','df_ever_overweight',\n",
    "        # 'df_ever_underweight','df_feedings_per_day','free_fed','df_diet_consistency',\n",
    "        # 'df_primary_diet_component','df_treats_frequency','df_weight_change_last_year'\n",
    "        # if don't know one of these, going to assume a no\n",
    "        joined_df['df_appetite_change_last_year'] = np.where(joined_df['df_appetite_change_last_year'] == 99, 0, 1)\n",
    "        joined_df['df_ever_malnourished'] = np.where(joined_df['df_ever_malnourished'] == 99, 0, 1)\n",
    "        joined_df['df_ever_overweight'] = np.where(joined_df['df_ever_overweight'] == 99, 0, 1)\n",
    "        joined_df['df_ever_underweight'] = np.where(joined_df['df_ever_underweight'] == 99, 0, 1)\n",
    "        joined_df['df_primary_diet_component'] = np.where(joined_df['df_primary_diet_component'] == 98, 8, 1)\n",
    "        \n",
    "        # 'df_feedings_per_day', 1 (ONCE) 2 (TWICE) 3 (3+) 4 (Free fed) -> breakout free fed as separate data point 'free_fed'\n",
    "        # break out the free fed:\n",
    "        # set up bool field for t/f 1/0 'free_fed'\n",
    "        joined_df['free_fed'] = (joined_df['df_feedings_per_day']==4).astype(int)\n",
    "        joined_df['df_feedings_per_day'] = np.where(joined_df['df_feedings_per_day'] == 4, 0, 1)\n",
    "        # added a feature\n",
    "        feat_num = feat_num + 1\n",
    "        \n",
    "    if 13 in data_cat:\n",
    "        # MEDICATIONS / PREVENTIVES'\n",
    "        # 'mp_dental_examination_frequency','mp_dental_brushing_frequency','mp_dental_treat_frequency',\n",
    "        # 'mp_dental_food_frequency','mp_dental_breath_freshener_frequency','mp_flea_and_tick_treatment',\n",
    "        # 'mp_heartworm_preventative'\n",
    "        joined_df['mp_flea_and_tick_treatment'] = joined_df['mp_flea_and_tick_treatment'].astype(int)\n",
    "        joined_df['mp_heartworm_preventative'] = joined_df['mp_heartworm_preventative'].astype(int)\n",
    "#         'mp_flea_and_tick_treatment', t/f\n",
    "#         'mp_heartworm_preventative', t/f\n",
    "    \n",
    "    # TEMPORAL\n",
    "    # 'hs_follow_up_ongoing','hs_required_surgery_or_hospitalization'\n",
    "    joined_df['hs_follow_up_ongoing'] = joined_df['hs_follow_up_ongoing'].astype(int)\n",
    "    # want 'hs_required_surgery_or_hospitalization' to be more linear in nature (ie. 1 is nothing, 4 is worse (surgery AND hospitalization))\n",
    "    for index, row in joined_df.iterrows(): \n",
    "        if row['hs_required_surgery_or_hospitalization'] == 1.0:\n",
    "            row['state'] = 3\n",
    "        elif row['hs_required_surgery_or_hospitalization'] == 3.0:\n",
    "            row['state'] = 4\n",
    "        elif row['hs_required_surgery_or_hospitalization'] == 4.0:\n",
    "            row['state'] = 1\n",
    "        else:\n",
    "            row['state'] = 2\n",
    "    \n",
    "    # convert prediction columns to traditional bool:\n",
    "    joined_df['hs_health_conditions_skin'] = np.where(joined_df['hs_health_conditions_skin'] == 2, 0, 1)\n",
    "    joined_df['hs_health_conditions_eye'] = np.where(joined_df['hs_health_conditions_eye'] == 2, 0, 1)\n",
    "    joined_df['hs_health_conditions_cancer'] = np.where(joined_df['hs_health_conditions_cancer'] == 2, 0, 1) \n",
    "    \n",
    "    # take these out of features list, bc need to process separately or aren't features\n",
    "#     feat_list.remove('dog_id')\n",
    "    # Need logic to take these out when applicable: 'hs_health_conditions_eye', 'hs_health_conditions_cancer' (but leave in others for features)\n",
    "    del feat_list[0:1]\n",
    "    feat_list.remove('dog_id')\n",
    "    # HAVE TO KEEP SKIN FOR TimeGANs!  But remove later where not wanted.\n",
    "    \n",
    "    final_df = joined_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # reduce to just dogs with 4+ health records\n",
    "#     # then look at the most common conditions & most common surgical/hospitalization scenarios\n",
    "#     sm_hl_df = heal_df[['dog_id', 'hs_condition_type','hs_required_surgery_or_hospitalization']]\n",
    "\n",
    "#     # MOST COMMON ISSUES:\n",
    "#     filter_df = sm_hl_df.loc[(sm_hl_df['hs_condition_type']==4) | (sm_hl_df['hs_condition_type']==3) | (sm_hl_df['hs_condition_type']==18) | (sm_hl_df['hs_condition_type']==16) | (sm_hl_df['hs_condition_type']==11) | (sm_hl_df['hs_condition_type']==1),['dog_id', 'hs_condition_type','hs_required_surgery_or_hospitalization']]\n",
    "\n",
    "#     # reduce to only dogs with seq_lenth+ of records # turning off for now\n",
    "#     if apply_seq_length==True:\n",
    "#         segmin_df = filter_df[filter_df.groupby('dog_id')['dog_id'].transform('size') > seq_length]\n",
    "#     else: \n",
    "#         segmin_df = filter_df\n",
    "\n",
    "#     # turn values into binary, adding new columns\n",
    "#     # if has 4, should be skin 1\n",
    "#     segmin_df['dis_skin'] = (segmin_df['hs_condition_type']==4).astype(int)\n",
    "#     segmin_df['dis_oral'] = (segmin_df['hs_condition_type']==3).astype(int)\n",
    "#     segmin_df['dis_trau'] = (segmin_df['hs_condition_type']==18).astype(int)\n",
    "#     segmin_df['dis_infe'] = (segmin_df['hs_condition_type']==16).astype(int)\n",
    "#     segmin_df['dis_bone'] = (segmin_df['hs_condition_type']==11).astype(int)\n",
    "#     segmin_df['dis_eyes'] = (segmin_df['hs_condition_type']==1).astype(int)\n",
    "    \n",
    "#     final_df = segmin_df\n",
    "#     if use_surg==True:\n",
    "#         segmin_df['surgery'] = (segmin_df['hs_required_surgery_or_hospitalization']==1).astype(int)\n",
    "#         segmin_df['hospitalized'] = (segmin_df['hs_required_surgery_or_hospitalization']==2).astype(int)\n",
    "#         segmin_df['both_surg_hosp'] = (segmin_df['hs_required_surgery_or_hospitalization']==3).astype(int)\n",
    "#         # remove old columns\n",
    "#         final_df = segmin_df[['dog_id','dis_skin','dis_oral','dis_trau','dis_infe','dis_bone','dis_eyes','surgery','hospitalized','both_surg_hosp','hs_required_surgery_or_hospitalization']] \n",
    "#     else:\n",
    "#         # remove old columns\n",
    "#         final_df = segmin_df[['dog_id','dis_skin','dis_oral','dis_trau','dis_infe','dis_bone','dis_eyes','hs_required_surgery_or_hospitalization']] \n",
    "\n",
    "#     feat_num = 0\n",
    "#     feat_list = []\n",
    "    \n",
    "    return final_df, feat_num, feat_list \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b87276-6980-4133-b366-240ffda6e52a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Time-series Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30984d2d-da93-4984-aaf3-f3e40a71b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min Max Normalizer\n",
    "\n",
    "def MinMaxScaler3(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce807b4-c14a-4383-bfab-8d584c881c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doggo_data_timeGANs(df):\n",
    "    \n",
    "    # remove non-feature columns\n",
    "    drop_cols = ['dog_id']\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # turn into segment vector\n",
    "    x = df.to_numpy()\n",
    "    \n",
    "    # Flip the data to make chronological data\n",
    "    # this reverses the order of the entire dataset, so newest in sequence is now first\n",
    "    # but data itself is still intact\n",
    "    x = x[::-1]\n",
    "    \n",
    "    # Min-Max Normalizer\n",
    "    x = MinMaxScaler3(x)\n",
    "    \n",
    "    # Build dataset\n",
    "    dataX = []\n",
    "    \n",
    "    for i in range(0, len(x) - seq_length):\n",
    "        _x = x[i:i + seq_length]\n",
    "        dataX.append(_x)\n",
    "        \n",
    "    # Mix Data (to make it similar to random sample)\n",
    "    idx = np.random.permutation(len(dataX))\n",
    "    \n",
    "    outputX = []\n",
    "    for i in range(len(dataX)):\n",
    "        outputX.append(dataX[idx[i]])\n",
    "        \n",
    "     # load final vars\n",
    "    X_ = outputX \n",
    "    \n",
    "    return outputX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9846f-bbc8-4f7a-923b-481d28d6b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Parameters\n",
    "# Experiments iterations\n",
    "Iteration = 2 # started with: 1\n",
    "Sub_Iteration = 3 # started with: 2\n",
    "\n",
    "## Data Loading\n",
    "seq_length = 4\n",
    "data_cats = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "gan_df, gan_feat_num, gan_feat_list = doggo_data_loading(seq_length=seq_length, data_cat=data_cats)\n",
    "dataX = doggo_data_timeGANs(gan_df)\n",
    "    \n",
    "print('doggo dataset is ready with features: ', gan_feat_num)\n",
    "# df.sample(28)\n",
    "# df.head(28)\n",
    "print(len(gan_feat_list))\n",
    "print(gan_feat_list)\n",
    "gan_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a85a05-1103-46c1-a90f-a63957493382",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newtork Parameters\n",
    "parameters = dict()\n",
    "\n",
    "parameters['hidden_dim'] = len(dataX[0][0,:]) * 4\n",
    "parameters['num_layers'] = 3 # started with: 2 (has to be at least 2!)\n",
    "parameters['iterations'] = 10000 # started with: 1000, low as 10\n",
    "parameters['batch_size'] = 128 # started with: 64, low as 2\n",
    "parameters['module_name'] = 'gru'   # Other options: 'lstm' or 'lstmLN'\n",
    "parameters['z_dim'] = len(dataX[0][0,:]) \n",
    "\n",
    "print('Parameters are ' + str(parameters))\n",
    "\n",
    "## Experiments\n",
    "# Output Initialization\n",
    "Discriminative_Score = list()\n",
    "Predictive_Score = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9192f68-15d1-43ae-99a7-81872da6c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TimeGANs model\n",
    "print('Start iterations') \n",
    "    \n",
    "# Each Iteration\n",
    "for it in range(Iteration):\n",
    "\n",
    "    \n",
    "    # Synthetic Data Generation\n",
    "    dataX_hat = timegan(dataX, parameters)   \n",
    "      \n",
    "    print('Finish Synthetic Data Generation')\n",
    "\n",
    "    ## Performance Metrics\n",
    "    \n",
    "    # 1. Discriminative Score\n",
    "    Acc = list()\n",
    "    for tt in range(Sub_Iteration):\n",
    "        Temp_Disc = discriminative_score_metrics (dataX, dataX_hat)\n",
    "        Acc.append(Temp_Disc)\n",
    "    \n",
    "    Discriminative_Score.append(np.mean(Acc))\n",
    "    \n",
    "    # 2. Predictive Performance\n",
    "    MAE_All = list()\n",
    "    for tt in range(Sub_Iteration):\n",
    "        MAE_All.append(predictive_score_metrics (dataX, dataX_hat))\n",
    "        \n",
    "    Predictive_Score.append(np.mean(MAE_All))    \n",
    "    \n",
    "print('Finish TimeGAN iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133b10-5344-4e7a-ba40-5f2fe9ae2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_Analysis (dataX, dataX_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee8a4e-44fa-428c-8fd7-783e3a9cd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_Analysis (dataX, dataX_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84223f-c74d-4856-8fa7-2386d383691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "print('Discriminative Score - Mean: ' + str(np.round(np.mean(Discriminative_Score),4)) + ', Std: ' + str(np.round(np.std(Discriminative_Score),4)))\n",
    "print('Predictive Score - Mean: ' + str(np.round(np.mean(Predictive_Score),4)) + ', Std: ' + str(np.round(np.std(Predictive_Score),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77a23e-da96-4644-a48a-2fa973f3267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataX[0:10]\n",
    "# for i in dataX[0:20]:\n",
    "#     print(i.shape)\n",
    "    \n",
    "# print(dataX[0].shape)\n",
    "# dataX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aebcdf-414f-435b-95e2-2879db9f7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataX[1].shape)\n",
    "# dataX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fe04b-9e80-424f-9640-fff41d8d039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataX[2].shape)\n",
    "dataX[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecadad0-0253-41b0-860c-8aebc2806f65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DISEASE TRAJECTORY STATE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5d729-687f-4629-a97d-cb04234098e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doggo_trajectory(df, feats, predict='skin'):\n",
    "    # print(assm_df.head(5))\n",
    "    \n",
    "    list_arr = []\n",
    "    arry_list = np.array([])\n",
    "    list_list = []\n",
    "    list_feat = []\n",
    "    cur_dog_id = df.iloc[0]['dog_id']\n",
    "    \n",
    "    df['state'] = 0.0\n",
    "    state_list = []\n",
    "    list_state_list = []\n",
    "    \n",
    "    for index, row in df.iterrows(): \n",
    "        # set up state lists - 'hs_health_conditions_?????'\n",
    "        # start with just skin - 'hs_health_conditions_skin', 1 YES, 2 NO\n",
    "        # set up state lists\n",
    "        if row['hs_health_conditions_skin'] == 1.0:\n",
    "            row['state'] = 1\n",
    "        elif row['hs_health_conditions_skin']  == 2.0:\n",
    "            row['state'] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        if cur_dog_id != row['dog_id']:\n",
    "            # print('list_list: ', list_list)\n",
    "            arry_list = np.array(list_list)\n",
    "            # print('arry_list: ', arry_list)\n",
    "            list_arr.append(arry_list)\n",
    "            list_state_list.append(state_list)\n",
    "        \n",
    "            # reset various fields\n",
    "            state_list = []\n",
    "            arry_list = np.array([]) # shouldn't need this, but jic\n",
    "            list_list = [] # reset the list of lists\n",
    "            cur_dog_id = row['dog_id']\n",
    "\n",
    "        # set up features list:\n",
    "        # hard code this for now, then go back and iterate it:\n",
    "        # [hs_general_health','hs_recent_hospitalization']\n",
    "        # [hs_follow_up_ongoing',hs_required_surgery_or_hospitalization]\n",
    "        # ['hs_health_conditions_skin','hs_health_conditions_eye','hs_health_conditions_cancer']\n",
    "        \n",
    "#         list_feat = [row['dis_skin'],row['dis_oral'], row['dis_trau'], row['dis_infe'], row['dis_bone'], row['dis_eyes']]\n",
    "        # list_feat = [row['hs_general_health'],row['hs_recent_hospitalization'], row['hs_follow_up_ongoing'], row['hs_required_surgery_or_hospitalization'], row['hs_health_conditions_eye'], row['hs_health_conditions_cancer']]\n",
    "        # print('pre feats list_feat', list_feat)\n",
    "        # print(feats)\n",
    "        for feat in feats:\n",
    "            # print(feat)\n",
    "            list_feat.append(row[feat])\n",
    "        # print('list_feat: ', list_feat)\n",
    "        # print('list_feat size: ', len(list_feat))\n",
    "        list_list.append(list_feat)\n",
    "        # print(list_list)\n",
    "        state_list.append(row['state'])\n",
    "        list_feat = [] # shouldn't need this, but jic\n",
    "    \n",
    "    # if predict=='skin':\n",
    "    #     drop_cols = ['hs_health_conditions_skin']\n",
    "    #     df.drop(drop_cols, axis=1)\n",
    "    # elif predict=='eye':\n",
    "    #     drop_cols = ['hs_health_conditions_eye']\n",
    "    #     df.drop(drop_cols, axis=1)            \n",
    "    # elif predict=='cancer':\n",
    "    #     drop_cols = ['hs_health_conditions_cancer']\n",
    "    #     df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # load final vars\n",
    "    X_ = list_arr\n",
    "    # arrX = np.array(X_)\n",
    "    # print('x shape: ', arrX.shape)\n",
    "    \n",
    "    S_ = list_state_list\n",
    "    # arrS = np.array(S_)\n",
    "    # print('x shape: ', arrS.shape)\n",
    "    \n",
    "#     print(arrX[0:20])\n",
    "#     print(arrS[0:20])\n",
    "    \n",
    "    return X_, S_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4360fd-1c8c-4061-97ad-9ce1b3fe5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # drop_cols = ['dog_id', 'hs_health_conditions_skin']\n",
    "    # df.drop(drop_cols, axis=1)\n",
    "\n",
    "#     if predict=='skin':\n",
    "#         joined_df['hs_health_conditions_skin'] = np.where(joined_df['hs_health_conditions_skin'] == 2, 0, 1)\n",
    "#     elif predict=='eye':\n",
    "#         joined_df['hs_health_conditions_eye'] = np.where(joined_df['hs_health_conditions_eye'] == 2, 0, 1)\n",
    "#     elif predict=='cancer':\n",
    "#         joined_df['hs_health_conditions_cancer'] = np.where(joined_df['hs_health_conditions_cancer'] == 2, 0, 1)\n",
    "\n",
    "# if predict=='skin':\n",
    "        #     row['state'] = row['hs_health_conditions_skin']\n",
    "        #     list_feat = [row['hs_health_conditions_eye'],row['hs_health_conditions_cancer']]\n",
    "        # elif predict=='eye':\n",
    "        #     row['state'] = row['hs_health_conditions_eye']\n",
    "        #     list_feat = [row['hs_health_conditions_skin'],row['hs_health_conditions_cancer']]           \n",
    "        # elif predict=='cancer':\n",
    "        #     row['state'] = row['hs_health_conditions_cancer']\n",
    "        #     list_feat = [row['hs_health_conditions_eye'],row['hs_health_conditions_skin']]\n",
    "        \n",
    "        # print('pre feats list_feat', list_feat)\n",
    "        # print(feats)\n",
    "        # for feat in feats:\n",
    "        #     # print(feat)\n",
    "        #     list_feat.append(row[feat])\n",
    "        # print(list_feat)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26292303-8cb8-4c0e-9d66-2da6ea061f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "seq_length = 4\n",
    "data_cats = [1,2,3,4,5,6,7,8,9,10,11,12,13] # \n",
    "disease = 'skin' # 'eye' 'cancer'\n",
    "\n",
    "# generate the data\n",
    "# df, feat_num, feat_list = doggo_data_loading(seq_length=seq_length, data_cat=data_cats) # [1,2,3,4,5,6,7,8,9,10,11,12,13] # [1,10,14]\n",
    "# final_df, feat_num, feat_list = doggo_data_loading(apply_seq_length=False, use_surg=False, data_cat=data_cats)\n",
    "assm_df, assm_feat_num, assm_feat_list = doggo_data_loading(seq_length=seq_length, apply_seq_length=True, data_cat=data_cats) #  apply_seq_length=True, use_surg=False, \n",
    "print(assm_feat_num, ' : ', assm_feat_list)\n",
    "# print(assm_df.shape)\n",
    "# FYI - can't get entire DF to run through without ASSM collapsing.  Max on minimum params is 21,000, larger params 29,400\n",
    "X_observations, true_states = generate_doggo_trajectory(assm_df[0:17000], feats=assm_feat_list, predict=disease)\n",
    "# [0:17000]\n",
    "# TODO adjust 0:20000 to as high as possible\n",
    "# keep adding categories!\n",
    "\n",
    "print(assm_df.columns)\n",
    "print(len(assm_df.columns))\n",
    "assm_df.head(10)\n",
    "# X_observations[0:3]\n",
    "# true_states[0:10]\n",
    "\n",
    "# ## Data Loading\n",
    "# seq_length = 4\n",
    "# data_cats = [1]\n",
    "\n",
    "# df, feat_num, feat_list = doggo_data_loading(seq_length=seq_length, data_cat=data_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58f523-588b-40af-8a12-8ff8fb70f650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b85b5-b528-4e7f-96b2-e01f94a16577",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_index = 3\n",
    "\n",
    "true_states[trajectory_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8800e2-d02c-42b6-bfab-07bdbc0d65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the hidden state data trajectories\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "t = list(range(len(true_states[trajectory_index])))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('Observations', color=color)\n",
    "ax1.plot(t, X_observations[trajectory_index], color=color, linewidth=5, alpha=0.2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Hidden states', color=color)  \n",
    "ax2.step(t, true_states[trajectory_index], color=color, linewidth=5)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assm_feat_num)\n",
    "model = attentive_state_space_model(num_states=2,\n",
    "                              maximum_seq_length=30, \n",
    "                              input_dim=assm_feat_num, # 6,  # started with: 6. tried 10, but broke, but that was original, adjust for how many features used in data\n",
    "                              rnn_type='LSTM',\n",
    "                              latent=True,\n",
    "                              generative=True,\n",
    "                              num_iterations=50,   # started with: 5\n",
    "                              num_epochs=3,  # started with: 2\n",
    "                              batch_size=100,  # started with: 10\n",
    "                              learning_rate=5*1e-4, \n",
    "                              num_rnn_hidden=100,  # started with: 5\n",
    "                              num_rnn_layers=1,\n",
    "                              dropout_keep_prob=None,\n",
    "                              num_out_hidden=100,  # started with: 5\n",
    "                              num_out_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c9c65-60ca-4f89-9251-89efc26604f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initial_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_means.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_observations[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_idx = 0\n",
    "state_inference, expected_observations, attention = model.predict([X_observations[predict_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(state_inference[predict_idx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "trajectory_index = 0\n",
    "\n",
    "t = list(range(len(true_states[trajectory_index])))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('MAP state estimate', color=color)\n",
    "ax1.plot(t, np.argmax(state_inference[trajectory_index], axis=1), color=color, linewidth=5)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('True states', color=color)  \n",
    "ax2.step(t, true_states[trajectory_index], color=color, linewidth=5)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "t = list(range(len(true_states[trajectory_index])))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('True observations', color=color)\n",
    "ax1.plot(t, X_observations[trajectory_index], color=color, linewidth=5, alpha=0.2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Predicted average observations', color=color)  \n",
    "ax2.plot(t, expected_observations[trajectory_index], color=color, linewidth=5, alpha=0.1)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention weights over time\n",
    "import seaborn as sns\n",
    "\n",
    "Attention_weights = []\n",
    "\n",
    "for w in range(len(attention[0])):\n",
    "    \n",
    "    Attention_weights.append(np.vstack((attention[0][w], np.zeros((len(attention[0][-1]) - len(attention[0][w]),1)))))\n",
    "\n",
    "\n",
    "Attention_weights = np.array(Attention_weights).reshape((len(attention[0][-1]), len(attention[0][-1])))[:state_inference[0].shape[0], :state_inference[0].shape[0]]\n",
    "\n",
    "mask = np.zeros_like(Attention_weights)\n",
    "\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(Attention_weights, mask=mask, vmax=.3, square=True)\n",
    "    ax.set_ylabel('Chronological time')\n",
    "    ax.set_xlabel('Previous time steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-efficiency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328bc950-ab4f-42ae-b7af-0df1f330a18a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TimeGANs Synthesized Data with ASSM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c2756-65e1-44b5-b072-c84a2c53bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab017f-629a-43c7-81b6-78a94445d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible order: Based on order in feat_list (generated when creating data) 46 items\n",
    "# ['hs_health_conditions_skin' (0/1), 'hs_health_conditions_eye' (0/1), 'hs_health_conditions_cancer' (0/1), \n",
    "# 'dd_breed_pure_or_mixed' (1/2), 'df_ever_underweight' (1/0), 'hs_recent_hospitalization', \n",
    "# 'dd_spayed_or_neutered', 'dd_spay_or_neuter_age', 'dd_insurance', \n",
    "# 'de_daytime_sleep_avg_hours', 'de_nighttime_sleep_avg_hours', 'de_eats_feces', \n",
    "# 'df_appetite', 'df_appetite_change_last_year', 'df_ever_malnourished', \n",
    "# 'df_ever_overweight', 'df_feedings_per_day', 'free_fed', 'mp_flea_and_tick_treatment', \n",
    "# 'mp_heartworm_preventative', 'df_primary_diet_component', 'mp_dental_brushing_frequency', \n",
    "# 'db_excitement_level_before_walk', 'db_excitement_level_before_car_ride', \n",
    "# 'db_aggression_level_approached_while_eating', 'db_aggression_level_on_leash_unknown_dog', \n",
    "# 'db_left_alone_barking_frequency', 'db_left_alone_restlessness_frequency', \n",
    "# 'db_fear_level_loud_noises', 'db_fear_level_unknown_human_touch', \n",
    "# 'db_attention_seeking_follows_humans_frequency', \n",
    "# 'db_attention_seeking_sits_close_to_humans_frequency', 'hs_general_health', \n",
    "# 'dd_age_years', 'dd_sex', 'dd_weight_range', 'de_eats_grass_frequency', \n",
    "# 'db_training_distraction_frequency', 'db_training_obeys_stay_command_frequency', \n",
    "# 'db_barks_frequency' (0.0-4.0), 'db_escapes_home_or_property_frequency (0.0-4.0)', 'db_playful_frequency' (0.0-4.0), \n",
    "# 'db_urinates_in_home_frequency' (0.0-4.0), 'hs_follow_up_ongoing' (1/0), \n",
    "# 'hs_required_surgery_or_hospitalization' (1/2/3/4), 'dd_combined_main_breed' (0-300 or so)]\n",
    "\n",
    "# another possibility: Based on order of df created from data creation: 47 items (including id)\n",
    "# Index(['dog_id', 'db_aggression_level_approached_while_eating' (0.0 - 4.0),\n",
    "#        'db_aggression_level_on_leash_unknown_dog (0.0 - 4.0)',\n",
    "#        'db_attention_seeking_follows_humans_frequency',\n",
    "#        'db_attention_seeking_sits_close_to_humans_frequency',\n",
    "#        'db_barks_frequency', 'db_escapes_home_or_property_frequency',\n",
    "#        'db_excitement_level_before_car_ride',\n",
    "#        'db_excitement_level_before_walk', 'db_fear_level_loud_noises',\n",
    "#        'db_fear_level_unknown_human_touch', 'db_left_alone_barking_frequency',\n",
    "#        'db_left_alone_restlessness_frequency', 'db_playful_frequency',\n",
    "#        'db_training_distraction_frequency',\n",
    "#        'db_training_obeys_stay_command_frequency',\n",
    "#        'db_urinates_in_home_frequency', 'dd_age_years',\n",
    "#        'dd_breed_pure_or_mixed', 'dd_insurance', 'dd_sex',\n",
    "#        'dd_spay_or_neuter_age', 'dd_spayed_or_neutered', 'dd_weight_range',\n",
    "#        'de_daytime_sleep_avg_hours', 'de_eats_feces',\n",
    "#        'de_eats_grass_frequency', 'de_nighttime_sleep_avg_hours',\n",
    "#        'df_appetite', 'df_appetite_change_last_year', 'df_ever_malnourished',\n",
    "#        'df_ever_overweight', 'df_ever_underweight', 'df_feedings_per_day',\n",
    "#        'df_primary_diet_component', 'hs_general_health',\n",
    "#        'hs_health_conditions_cancer', 'hs_health_conditions_eye',\n",
    "#        'hs_health_conditions_skin', 'hs_recent_hospitalization',\n",
    "#        'mp_dental_brushing_frequency', 'mp_flea_and_tick_treatment',\n",
    "#        'mp_heartworm_preventative', 'hs_follow_up_ongoing',\n",
    "#        'hs_required_surgery_or_hospitalization', 'dd_combined_main_breed',\n",
    "#        'free_fed'],\n",
    "#       dtype='object')\n",
    "\n",
    "# first row output of synthesized data: (normalized!)\n",
    "# 0.132518 (breed?) 0.0 (1-4?) 0.00 (1-4) 0.25 (1-4) 0.75 (1-4?) 0.0 (1-4?) 0.0 (1-4) 0.50 (1-4) 0.25 (1-4) 0.00 (1-4) ... 1.0 0.0 0.75 1.0 0.0 1.0 1.000000 0.010020 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed989f8b-f894-4017-983a-1c792e8c1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae06bf1-f1a5-4ad2-afb0-1591cb53d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn_df = pd.DataFrame(dataX, columns=assm_feat_list) # ,dtype=float\n",
    "list_of_lists = []\n",
    "\n",
    "for arr in dataX_hat: #dataX\n",
    "    # print(arr)\n",
    "    for outer_lst in arr:\n",
    "        # print(outer_lst)\n",
    "        # print(outer_lst.shape)\n",
    "        list_of_lists.append(outer_lst)\n",
    "        # for d in outer_lst:\n",
    "        #     print(d)\n",
    "        \n",
    "# print(len(assm_feat_list))\n",
    "# print(assm_feat_list)\n",
    "syn_df = pd.DataFrame(list_of_lists) # , columns=assm_feat_list)\n",
    "\n",
    "# print(syn_df.shape)\n",
    "\n",
    "class NumAdv:\n",
    "    def __iter__(self):\n",
    "        self.a = int(1)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        x = self.a\n",
    "        self.a += 1\n",
    "        if self.a % 2==0:\n",
    "            fin = (x / 2)\n",
    "        else:\n",
    "            fin = ((x - 1) / 2)\n",
    "        return fin * 2\n",
    "\n",
    "def get_id(df):\n",
    "    dog_id = ((next(myiter)) + 100000) # original dog ids max at 92582\n",
    "    return dog_id\n",
    "\n",
    "# add a non-consecutive id for every 2 rows\n",
    "idclass = NumAdv()\n",
    "myiter = iter(idclass)\n",
    "syn_df['dog_id'] = syn_df.apply(get_id, axis = 1)\n",
    "\n",
    "syn_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebd360-df0f-4a56-8239-85c466ecefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doggo_trajectory_syn(df, feats, predict='skin'):\n",
    "    # print(assm_df.head(5))\n",
    "    \n",
    "    list_arr = []\n",
    "    arry_list = np.array([])\n",
    "    list_list = []\n",
    "    list_feat = []\n",
    "    cur_dog_id = df.iloc[0]['dog_id']\n",
    "    \n",
    "    df['state'] = 0.0\n",
    "    state_list = []\n",
    "    list_state_list = []\n",
    "    \n",
    "    for index, row in df.iterrows(): \n",
    "        # set up state lists - 'hs_health_conditions_?????'\n",
    "        # start with just skin - 'hs_health_conditions_skin', 1 YES, 2 NO\n",
    "        # set up state lists\n",
    "        if row[39] <= 0.5: # 'hs_health_conditions_skin'\n",
    "            row['state'] = 1\n",
    "        elif row[39]  > 0.5: # 'hs_health_conditions_skin'\n",
    "            row['state'] = 0\n",
    "        \n",
    "        if cur_dog_id != row['dog_id']:\n",
    "            # print('list_list: ', list_list)\n",
    "            arry_list = np.array(list_list)\n",
    "            # print('arry_list: ', arry_list)\n",
    "            list_arr.append(arry_list)\n",
    "            list_state_list.append(state_list)\n",
    "        \n",
    "            # reset various fields\n",
    "            state_list = []\n",
    "            arry_list = np.array([]) # shouldn't need this, but jic\n",
    "            list_list = [] # reset the list of lists\n",
    "            cur_dog_id = row['dog_id']\n",
    "\n",
    "        # set up features list:\n",
    "        # hard code this for now, then go back and iterate it:\n",
    "        # [hs_general_health','hs_recent_hospitalization']\n",
    "        # [hs_follow_up_ongoing',hs_required_surgery_or_hospitalization]\n",
    "        # ['hs_health_conditions_skin','hs_health_conditions_eye','hs_health_conditions_cancer']\n",
    "        \n",
    "#         list_feat = [row['dis_skin'],row['dis_oral'], row['dis_trau'], row['dis_infe'], row['dis_bone'], row['dis_eyes']]\n",
    "        # list_feat = [row['hs_general_health'],row['hs_recent_hospitalization'], row['hs_follow_up_ongoing'], row['hs_required_surgery_or_hospitalization'], row['hs_health_conditions_eye'], row['hs_health_conditions_cancer']]\n",
    "        # print('pre feats list_feat', list_feat)\n",
    "        # print(feats)\n",
    "        for feat in range(0,39):\n",
    "            # print(feat)\n",
    "            list_feat.append(row[feat])\n",
    "        # have to skip skin\n",
    "        for feat in range(40,46):\n",
    "            # print(feat)\n",
    "            list_feat.append(row[feat])\n",
    "            \n",
    "        # print('list_feat: ', list_feat)\n",
    "        # print('list_feat size: ', len(list_feat))\n",
    "        list_list.append(list_feat)\n",
    "        state_list.append(row[39])\n",
    "        list_feat = [] # shouldn't need this, but jic\n",
    "    \n",
    "    # if predict=='skin':\n",
    "    #     drop_cols = ['hs_health_conditions_skin']\n",
    "    #     df.drop(drop_cols, axis=1)\n",
    "    # elif predict=='eye':\n",
    "    #     drop_cols = ['hs_health_conditions_eye']\n",
    "    #     df.drop(drop_cols, axis=1)            \n",
    "    # elif predict=='cancer':\n",
    "    #     drop_cols = ['hs_health_conditions_cancer']\n",
    "    #     df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # load final vars\n",
    "    X_ = list_arr\n",
    "    # arrX = np.array(X_)\n",
    "    # print('x shape: ', arrX.shape)\n",
    "    \n",
    "    S_ = list_state_list\n",
    "    # arrS = np.array(S_)\n",
    "    # print('x shape: ', arrS.shape)\n",
    "    \n",
    "#     print(arrX[0:20])\n",
    "#     print(arrS[0:20])\n",
    "    \n",
    "    return X_, S_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526c5d8-fae7-4576-9c03-4c7ea3ef1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "\n",
    "# FYI - can't get entire DF to run through without ASSM collapsing.  Max on minimum params is 21,000, larger params 29,400\n",
    "X_observations_syn, true_states_syn = generate_doggo_trajectory_syn(syn_df[0:17300], feats=assm_feat_list, predict=disease)\n",
    "\n",
    "# TODO adjust 0:20000 to as high as possible\n",
    "# keep adding categories!\n",
    "\n",
    "# assm_df.head(10)\n",
    "# X_observations[0:3]\n",
    "# true_states[0:10]\n",
    "\n",
    "# ## Data Loading\n",
    "# seq_length = 4\n",
    "# data_cats = [1]\n",
    "\n",
    "# df, feat_num, feat_list = doggo_data_loading(seq_length=seq_length, data_cat=data_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80271a92-cbfe-4529-90fb-2afe2e2708af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_index_syn = 3\n",
    "\n",
    "true_states_syn[trajectory_index_syn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44470a-d659-4fa9-ab2b-ab9a074f33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the hidden state data trajectories\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "t_syn = list(range(len(true_states_syn[trajectory_index_syn])))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('Observations', color=color)\n",
    "ax1.plot(t_syn, X_observations_syn[trajectory_index_syn], color=color, linewidth=5, alpha=0.2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Hidden states', color=color)  \n",
    "ax2.step(t_syn, true_states_syn[trajectory_index_syn], color=color, linewidth=5)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c6ada-ce33-4579-ae0f-9c30120c0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assm_feat_num)\n",
    "model_syn = attentive_state_space_model(num_states=2,\n",
    "                              maximum_seq_length=30, \n",
    "                              input_dim=assm_feat_num-1, # 6,  # started with: 6. tried 10, but broke, but that was original, adjust for how many features used in data\n",
    "                              rnn_type='LSTM',\n",
    "                              latent=True,\n",
    "                              generative=True,\n",
    "                              num_iterations=50,   # started with: 5\n",
    "                              num_epochs=3,  # started with: 2\n",
    "                              batch_size=100,  # started with: 10\n",
    "                              learning_rate=5*1e-4, \n",
    "                              num_rnn_hidden=100,  # started with: 5\n",
    "                              num_rnn_layers=1,\n",
    "                              dropout_keep_prob=None,\n",
    "                              num_out_hidden=100,  # started with: 5\n",
    "                              num_out_layers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10658729-4378-4977-926f-dc385afaff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn.fit(X_observations_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1dde92-6669-4b37-8708-e69c4c743c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn.initial_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c79d5a-1ca7-4055-bf65-aaf97382968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60971cc-005d-4294-81ef-5a738e0e8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn.state_means.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b6524-67c5-4ccb-a9d5-127679b4f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn.state_covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5048d6-be9a-4662-8eb7-bd03d96cc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_observations_syn[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d440498-e0dd-4352-a51d-f0a6eb2fd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_idx_syn = 0\n",
    "state_inference_syn, expected_observations_syn, attention_syn = model_syn.predict([X_observations_syn[predict_idx_syn]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3ea70-4c41-4a46-ab86-ef2198061538",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(state_inference_syn[predict_idx_syn], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464578c-981f-4b94-ac06-df8c08b70999",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "trajectory_index_syn = 0\n",
    "t_syn = list(range(len(true_states_syn[trajectory_index_syn])))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('MAP state estimate', color=color)\n",
    "ax1.plot(t_syn, np.argmax(state_inference_syn[trajectory_index_syn], axis=1), color=color, linewidth=5)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('True states', color=color)  \n",
    "ax2.step(t_syn, true_states_syn[trajectory_index_syn], color=color, linewidth=5)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72dc51-30e8-4232-9a6f-21ab56571c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "t_syn = list(range(len(true_states_syn[trajectory_index_syn])))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('True observations', color=color)\n",
    "ax1.plot(t_syn, X_observations_syn[trajectory_index_syn], color=color, linewidth=5, alpha=0.2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Predicted average observations', color=color)  \n",
    "ax2.plot(t_syn, expected_observations_syn[trajectory_index_syn], color=color, linewidth=5, alpha=0.1)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35e852-eb60-4416-9f38-846f12fffc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention weights over time\n",
    "import seaborn as sns\n",
    "Attention_weights_syn = []\n",
    "for w in range(len(attention_syn[0])):\n",
    "    Attention_weights_syn.append(np.vstack((attention_syn[0][w], np.zeros((len(attention_syn[0][-1]) - len(attention_syn[0][w]),1)))))\n",
    "Attention_weights_syn = np.array(Attention_weights_syn).reshape((len(attention_syn[0][-1]), len(attention_syn[0][-1])))[:state_inference_syn[0].shape[0], :state_inference_syn[0].shape[0]]\n",
    "mask_syn = np.zeros_like(Attention_weights_syn)\n",
    "mask_syn[np.triu_indices_from(mask_syn)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(Attention_weights_syn, mask=mask_syn, vmax=.3, square=True)\n",
    "    ax.set_ylabel('Chronological time')\n",
    "    ax.set_xlabel('Previous time steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d19a13-256f-49b8-89ee-416255fd0fdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using TimeGANs to Oversample Minority Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166f3fa-bdc3-4b88-9069-36ad1cdaeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments iterations\n",
    "Iteration = 2\n",
    "Sub_Iteration = 3\n",
    "\n",
    "# take original TimeGANs loaded data as usual, \n",
    "#   but then filter by 'hs_health_conditions_skin'==1\n",
    "#   before passing into TimeGANs \n",
    "print('current df size: ', gan_df.shape)\n",
    "gan_df_min = gan_df[gan_df['hs_health_conditions_skin'] == 1]\n",
    "print('new df size: ', gan_df_min.shape) # should be about 85% smaller?\n",
    "\n",
    "# so run original timeGANS data changer, but on smaller df\n",
    "dataX_min = doggo_data_timeGANs(gan_df_min)\n",
    "\n",
    "print('minority doggo dataset is ready')\n",
    "# df.sample(28)\n",
    "# df.head(28)\n",
    "gan_df_min.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee714767-81bb-496b-b36a-f599ada325b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newtork Parameters\n",
    "parameters_min = dict()\n",
    "\n",
    "parameters_min['hidden_dim'] = len(dataX_min[0][0,:]) * 4\n",
    "parameters_min['num_layers'] = 3 # started with: 2 (has to be at least 2!)\n",
    "parameters_min['iterations'] = 10000 # started with: 1000, low as 10\n",
    "parameters_min['batch_size'] = 128 # started with: 64, low as 2\n",
    "parameters_min['module_name'] = 'gru'   # Other options: 'lstm' or 'lstmLN'\n",
    "parameters_min['z_dim'] = len(dataX_min[0][0,:]) \n",
    "\n",
    "print('Minority class parameters are ' + str(parameters_min))\n",
    "\n",
    "## Experiments\n",
    "# Output Initialization\n",
    "Discriminative_Score_min = list()\n",
    "Predictive_Score_min = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe69d5-efc1-4505-a1e5-543b8a455239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TimeGANs model\n",
    "print('Start iterations') \n",
    "    \n",
    "# Each Iteration\n",
    "for it in range(Iteration):\n",
    "\n",
    "    \n",
    "    # Synthetic Data Generation\n",
    "    dataX_hat_min = timegan(dataX_min, parameters_min)   \n",
    "      \n",
    "    print('Finish Synthetic Data Generation')\n",
    "\n",
    "    ## Performance Metrics\n",
    "    \n",
    "    # 1. Discriminative Score\n",
    "    Acc_min = list()\n",
    "    for tt in range(Sub_Iteration):\n",
    "        Temp_Disc_min = discriminative_score_metrics(dataX_min, dataX_hat_min)\n",
    "        Acc_min.append(Temp_Disc_min)\n",
    "    \n",
    "    Discriminative_Score_min.append(np.mean(Acc_min))\n",
    "    \n",
    "    # 2. Predictive Performance\n",
    "    MAE_All_min = list()\n",
    "    for tt in range(Sub_Iteration):\n",
    "        MAE_All_min.append(predictive_score_metrics(dataX_min, dataX_hat_min))\n",
    "        \n",
    "    Predictive_Score_min.append(np.mean(MAE_All_min))    \n",
    "    \n",
    "print('Finish TimeGAN iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2701e-96f0-4a6b-af2d-95d04f9bdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_Analysis(dataX_min, dataX_hat_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee49d0f-7542-4016-aea5-b90816b8427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_Analysis(dataX_min, dataX_hat_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c57ba9-3283-4d02-a148-02710c1a14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "print('Minority class discriminative Score - Mean: ' + str(np.round(np.mean(Discriminative_Score_min),4)) + ', Std: ' + str(np.round(np.std(Discriminative_Score_min),4)))\n",
    "print('Minority class  predictive Score - Mean: ' + str(np.round(np.mean(Predictive_Score_min),4)) + ', Std: ' + str(np.round(np.std(Predictive_Score_min),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdf5a3-e13a-45fe-8a6a-6b68f2ef3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('new dogs: ', len(dataX_hat_min))\n",
    "print('meet scruffy: ', dataX_hat_min[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2efdd-eea8-4d28-998e-c395fe757c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4\n",
    "data_cats = [1,2,3,4,5,6,7,8,9,10,11,12,13] # \n",
    "disease = 'skin'\n",
    "\n",
    "# generate the data\n",
    "# df, feat_num, feat_list = doggo_data_loading(seq_length=seq_length, data_cat=data_cats) # [1,2,3,4,5,6,7,8,9,10,11,12,13] # [1,10,14]\n",
    "# final_df, feat_num, feat_list = doggo_data_loading(apply_seq_length=False, use_surg=False, data_cat=data_cats)\n",
    "orig_df, orig_feat_num, orig_feat_list = doggo_data_loading(seq_length=seq_length, apply_seq_length=True, data_cat=data_cats) #  apply_seq_length=True, use_surg=False, \n",
    "print(orig_feat_num, ' : ', orig_feat_list)\n",
    "\n",
    "print(orig_df.shape)\n",
    "orig_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df72d3-b593-4c62-b8b3-42d791e6304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = \"dog_id\"\n",
    "# max_x = orig_df.loc[orig_df[col].idxmax()]\n",
    "# print(\"Maximum value of column \", col, \" and its corresponding row values:\\n\", max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5b023-92e1-4e5f-a1f7-32009b364dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sync_data is the dataX_hat output from the TimeGANs\n",
    "# list_of_lists = []\n",
    "# for arr in dataX_hat_min:  # use sync_data in func\n",
    "# # print(arr)\n",
    "#     for outer_lst in arr:\n",
    "#         # print(outer_lst)\n",
    "#         # print(outer_lst.shape)\n",
    "#         list_of_lists.append(outer_lst)\n",
    "\n",
    "# sync_df = pd.DataFrame(list_of_lists)\n",
    "\n",
    "# idclass = NumAdv()\n",
    "# myiter = iter(idclass)\n",
    "# sync_df['dog_id'] = sync_df.apply(get_id, axis = 1)\n",
    "\n",
    "# # get a list of columns\n",
    "# cols = list(sync_df)\n",
    "# cols.insert(0, cols.pop(cols.index('dog_id')))\n",
    "# # cols\n",
    "\n",
    "# sync_df = sync_df.loc[:, cols]\n",
    "# # # sync_df\n",
    "\n",
    "# print(orig_df.columns)\n",
    "# print(len(orig_df.columns))\n",
    "# sync_df.columns = orig_df.columns\n",
    "\n",
    "# print(sync_df.shape)\n",
    "# sync_df.head(10)\n",
    "\n",
    "# MOVED TO FUNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042847a-c04f-4f78-af6f-92cb6884f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine new datapoints with existing dataset\n",
    "def generate_doggo_trajectory_comb(orig_df, sync_data, feats, predict='skin'):\n",
    "    # print(assm_df.head(5))\n",
    "    \n",
    "    # syn_data is the dataX_hat output from the TimeGANs\n",
    "    list_of_lists = []\n",
    "    for arr in sync_data:\n",
    "        # print(arr)\n",
    "        for outer_lst in arr:\n",
    "            # print(outer_lst)\n",
    "            # print(outer_lst.shape)\n",
    "            list_of_lists.append(outer_lst)\n",
    "        \n",
    "    sync_df = pd.DataFrame(list_of_lists)\n",
    "\n",
    "    idclass = NumAdv()\n",
    "    myiter = iter(idclass)\n",
    "    sync_df['dog_id'] = sync_df.apply(get_id, axis = 1)\n",
    "\n",
    "    # move dog_id to front of columns to align with orig_df\n",
    "    cols = list(sync_df)\n",
    "    cols.insert(0, cols.pop(cols.index('dog_id')))\n",
    "    sync_df = sync_df.loc[:, cols]\n",
    "\n",
    "    # assign col names to sync data\n",
    "    sync_df.columns = orig_df.columns\n",
    "\n",
    "    # join the 2 dataframes\n",
    "    comb_df = pd.concat([orig_df, sync_df])\n",
    "    print('combined df size: ', comb_df.shape)\n",
    "    \n",
    "    list_arr = []\n",
    "    arry_list = np.array([])\n",
    "    list_list = []\n",
    "    list_feat = []\n",
    "    cur_dog_id = comb_df.iloc[0]['dog_id']\n",
    "    \n",
    "    comb_df['state'] = 0.0\n",
    "    state_list = []\n",
    "    list_state_list = []\n",
    "    \n",
    "    for index, row in comb_df.iterrows(): \n",
    "        # set up state lists - 'hs_health_conditions_?????'\n",
    "        # start with just skin - 'hs_health_conditions_skin', 1 YES, 2 NO\n",
    "        # set up state lists\n",
    "        if row['hs_health_conditions_skin'] <= 0.5: # 'hs_health_conditions_skin'\n",
    "            row['state'] = 1\n",
    "        elif row['hs_health_conditions_skin']  > 0.5: # 'hs_health_conditions_skin'\n",
    "            row['state'] = 0\n",
    "        \n",
    "        if cur_dog_id != row['dog_id']:\n",
    "            # print('list_list: ', list_list)\n",
    "            arry_list = np.array(list_list)\n",
    "            # print('arry_list: ', arry_list)\n",
    "            list_arr.append(arry_list)\n",
    "            list_state_list.append(state_list)\n",
    "        \n",
    "            # reset various fields\n",
    "            state_list = []\n",
    "            arry_list = np.array([]) # shouldn't need this, but jic\n",
    "            list_list = [] # reset the list of lists\n",
    "            cur_dog_id = row['dog_id']\n",
    "\n",
    "        # set up features list:\n",
    "        # hard code this for now, then go back and iterate it:\n",
    "        # [hs_general_health','hs_recent_hospitalization']\n",
    "        # [hs_follow_up_ongoing',hs_required_surgery_or_hospitalization]\n",
    "        # ['hs_health_conditions_skin','hs_health_conditions_eye','hs_health_conditions_cancer']\n",
    "        \n",
    "#         list_feat = [row['dis_skin'],row['dis_oral'], row['dis_trau'], row['dis_infe'], row['dis_bone'], row['dis_eyes']]\n",
    "        # list_feat = [row['hs_general_health'],row['hs_recent_hospitalization'], row['hs_follow_up_ongoing'], row['hs_required_surgery_or_hospitalization'], row['hs_health_conditions_eye'], row['hs_health_conditions_cancer']]\n",
    "        # print('pre feats list_feat', list_feat)\n",
    "        # print(feats)\n",
    "        for feat in feats:\n",
    "            # print(feat)\n",
    "            list_feat.append(row[feat])\n",
    "        # print('list_feat: ', list_feat)\n",
    "        # print('list_feat size: ', len(list_feat))\n",
    "        list_list.append(list_feat)\n",
    "        state_list.append(row['state'])\n",
    "        list_feat = [] # shouldn't need this, but jic\n",
    "    \n",
    "    # if predict=='skin':\n",
    "    #     drop_cols = ['hs_health_conditions_skin']\n",
    "    #     df.drop(drop_cols, axis=1)\n",
    "    # elif predict=='eye':\n",
    "    #     drop_cols = ['hs_health_conditions_eye']\n",
    "    #     df.drop(drop_cols, axis=1)            \n",
    "    # elif predict=='cancer':\n",
    "    #     drop_cols = ['hs_health_conditions_cancer']\n",
    "    #     df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # load final vars\n",
    "    X_ = list_arr\n",
    "    # arrX = np.array(X_)\n",
    "    # print('x shape: ', arrX.shape)\n",
    "    \n",
    "    S_ = list_state_list\n",
    "    # arrS = np.array(S_)\n",
    "    # print('x shape: ', arrS.shape)\n",
    "    \n",
    "#     print(arrX[0:20])\n",
    "#     print(arrS[0:20])\n",
    "    \n",
    "    return X_, S_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75432f3d-a71e-48ed-9ee0-4503298773b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI - can't get entire DF to run through without ASSM collapsing.  Max on minimum params is 21,000, larger params 29,400\n",
    "# undersample the majority case. keep all minority case:\n",
    "# TODO come back to this!\n",
    "\n",
    "# combine and form data for model\n",
    "X_observations_comb, true_states_comb = generate_doggo_trajectory_comb(orig_df[0:17000], dataX_hat_min, feats=assm_feat_list, predict=disease)\n",
    "\n",
    "# TODO adjust 0:20000 to as high as possible\n",
    "# keep adding categories!\n",
    "\n",
    "# assm_df.head(20)\n",
    "# X_observations[0:3]\n",
    "# true_states[0:10]\n",
    "\n",
    "# ## Data Loading\n",
    "# seq_length = 4\n",
    "# data_cats = [1]\n",
    "\n",
    "# df, feat_num, feat_list = doggo_data_loading(seq_length=seq_length, data_cat=data_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3eb2b-d85f-42bc-bd85-a511c4c46168",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_index_comb = 3\n",
    "\n",
    "true_states_comb[trajectory_index_comb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd073a-ee87-4022-ba7d-edd08afc63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the hidden state data trajectories\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "t_comb = list(range(len(true_states_comb[trajectory_index_comb])))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('Observations', color=color)\n",
    "ax1.plot(t_comb, X_observations_comb[trajectory_index_comb], color=color, linewidth=5, alpha=0.2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Hidden states', color=color)  \n",
    "ax2.step(t_comb, true_states_comb[trajectory_index_comb], color=color, linewidth=5)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b96686-1259-4065-b116-9986cbc264d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d617d19-7d32-4eaa-88c8-38a5635c9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = orig_feat_num\n",
    "print(dims)\n",
    "model_comb = attentive_state_space_model(num_states=2,\n",
    "                              maximum_seq_length=30, \n",
    "                              input_dim=dims, # 6,  # started with: 6. tried 10, but broke, but that was original, adjust for how many features used in data\n",
    "                              rnn_type='LSTM',\n",
    "                              latent=True,\n",
    "                              generative=True,\n",
    "                              num_iterations=50,   # started with: 5\n",
    "                              num_epochs=3,  # started with: 2\n",
    "                              batch_size=100,  # started with: 10\n",
    "                              learning_rate=5*1e-4, \n",
    "                              num_rnn_hidden=100,  # started with: 5\n",
    "                              num_rnn_layers=1,\n",
    "                              dropout_keep_prob=None,\n",
    "                              num_out_hidden=100,  # started with: 5\n",
    "                              num_out_layers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53c0a1-363d-42c6-90e7-a8ccb5bac829",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb.fit(X_observations_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504be271-9bd1-4f56-9dae-a260adeaef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb.initial_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fce35-ce1c-4278-ad03-09e42abbc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a94309-6343-4d51-b217-f59eac5ad8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb.state_means.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83588e-5a88-49ac-a880-b71fca2b8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb.state_covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313c284-99f7-4011-948a-e9080919d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_observations_comb[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e7ec6-96b4-454b-80fa-877bfface36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_idx_comb = 0\n",
    "state_inference_comb, expected_observations_comb, attention_comb = model_comb.predict([X_observations_comb[predict_idx_comb]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba3691-9b9e-42b2-8c2e-42d8073c57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(state_inference_comb[predict_idx_comb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b271f-e05f-44b5-bcda-7c5066b70468",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "trajectory_index_comb = 0\n",
    "t_comb = list(range(len(true_states_comb[trajectory_index_comb])))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('MAP state estimate', color=color)\n",
    "ax1.plot(t_comb, np.argmax(state_inference_comb[trajectory_index_comb], axis=1), color=color, linewidth=5)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('True states', color=color)  \n",
    "ax2.step(t_comb, true_states_comb[trajectory_index_comb], color=color, linewidth=5)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5329757-9d9e-4068-8fe4-32dd3c78197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "t_comb = list(range(len(true_states_comb[trajectory_index_comb])))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('True observations', color=color)\n",
    "ax1.plot(t_comb, X_observations_comb[trajectory_index_comb], color=color, linewidth=5, alpha=0.2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Predicted average observations', color=color)  \n",
    "ax2.plot(t_comb, expected_observations_comb[trajectory_index_comb], color=color, linewidth=5, alpha=0.1)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883f0d4-008e-4b57-99b4-e84dbf834215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention weights over time\n",
    "import seaborn as sns\n",
    "Attention_weights_comb = []\n",
    "for w in range(len(attention_comb[0])):\n",
    "    Attention_weights_comb.append(np.vstack((attention_comb[0][w], np.zeros((len(attention_comb[0][-1]) - len(attention_comb[0][w]),1)))))\n",
    "Attention_weights_comb = np.array(Attention_weights_comb).reshape((len(attention_comb[0][-1]), len(attention_comb[0][-1])))[:state_inference_comb[0].shape[0], :state_inference_comb[0].shape[0]]\n",
    "mask_comb = np.zeros_like(Attention_weights_comb)\n",
    "mask_comb[np.triu_indices_from(mask_comb)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(Attention_weights_comb, mask=mask_comb, vmax=.3, square=True)\n",
    "    ax.set_ylabel('Chronological time')\n",
    "    ax.set_xlabel('Previous time steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900ee83-af31-4e70-9434-4619a31a192a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
